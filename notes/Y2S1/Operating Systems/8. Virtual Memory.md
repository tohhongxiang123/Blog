# Virtual Memory

# Swapping

![Swapping processes in and out of memory](https://binaryterms.com/wp-content/uploads/2019/08/Swapping.jpg)

- A process can be swapped temporarily out of memory to a backing store, and then brought back in to memory for continued execution
- A backing store is a fast disk large enough to accommodate copies of alll memory images for all users; must provide direct access to these memory images
- Major part of swap time is transfer time: The total transfer time is directly proportional to the amount of memory swapped
- Swapping is used to
    - Make space available for processes that require more memory
    - Increase the degree of multiprogramming

# Virtual Memory Support

- So far we assumed that the whole program is loaded into the memory, at least at some point of time
    - Restriction: Overall program size including all library routines that may be linked at runtime must be restricted to the size of physical memory
- To remove this restriction, the logical memory size requirement of the user program must be de-coupled from the size of the physical memory space
- This is what virtual memory aims to achieve
    - The size of virtual memory is limited by the addressing scheme of a computer (e.g. how many bits are used to represent an address), not by the actual size of physical memory

![Page table](https://i.stack.imgur.com/e2NNe.png)

- Virtual memory support: Separation of user logical memory from the physical memory
- Implication sof virtual memory support
    - Logical address space can be much larger than available memory
    - Only part of the program needs to be in memory for execution -> Greater degree of multiprogramming -> Increased CPU utilisation
    - Need to allow pages to be paged in and out
- Virtual memory is implemented mostly using demand paging (demand segmentation is also used)

# Demand Paging

Demand paging is a method of virtual memory management. The OS copies a disk page in to physical memory only if an attempt is made to access it, and that page is not already in memory (i.e. a page fault occurs)

- Memory requirement of a process is divided into pages
- Logical address `<page number, offset>` is a reference generated for the page -> page is needed
- With each page table entry, a valid-invalid bit is associated, with values either `i` or `v`
    - `v`: data is in memory, page table entry contains the location of the page in memory (frame number)
    - `i`: data is not in memory, page table entry contains the address of the page on disk
    - Initially valid-invalid bit is set to `i` on all entries (This means that initially, there are no pages in memory)
    - During address translation, if valid-invalid bit in page table entry is `i` -> Page fault (we are required to bring the page into memory)

## Procedure

![](https://s3.ap-south-1.amazonaws.com/afteracademy-server-uploads/what-are-demand-paging-and-pre-paging-demand-paging-diagram-aaf784972c29c344.jpg)

1. Attempt to access page by checking page table
2. If page is valid (in memory), continue processing instructions as normal
3. If page is invalid, a **page-fault trap** occurs, interrupting the current process (a context switch also occurs)
4. OS checks if memory reference in page table is a valid reference to a location on secondary memory
5. If invalid reference, process is terminated (**illegal memory access**)
6. Page in the data from the secondary memory into the main memory
7. Schedule disk operation to read the desired page into main memory
8. Update page table
9. Restart the instruction that was interrupted by the operating system trap (context switch back)

## Performance of Demand Paging

- Let $p$ be the probability of a page fault occuring ($0 \leq p \leq 1$)
    - If $p = 0$, no page fault occurs
    - If $p=1$, every reference causes a fault
- Effective access time (EAT),
    $$
        EAT = (1-p)(\text{memory access time}) + p(\text{page fault time})
    $$
- In practice, memory access time < page fault time

# Page Replacement

- When a page fault occurs, what happens if there is no free frame?
- Solution: Find some page in memory that is not really in use, and page it out (replace page)
- If no free frame is available, then two page transfers may in necessary (page in, and page out). This increases the page fault time even more
- Use a page replacement algorithm to locate the page to be replaced. Desirably, the algorithm should result in minimum number of page faults

## General Page Replacement Procedure

1. Find a victim page using place replacement algorithm and write the victim page to the backing store (page-out)
2. Change the page table entry of the victim page to invalid
3. Bring in the desired page into the newly freed frame (page-in)
4. Update the page table entry for the new page (set the frame to the location of the page in, and the valid bit should be set)

Note: When replacing pages, only dirty pages are actually written to the backing store
- Dirty pages are pages that have been changed, but have not yet been written out to disk
- Non-dirty pages in the page cache have identical copies in secondary storage
- Since non-dirty pages have not been modified, we can minimise overhead of page replacement
- Dirty pages are identified witha modify (or dirty) bit
- Page replacement completes separation between logical memory and physical memory - large virtual memory can be provided on smaller physical memory

## Page Replacement Algorithms

Page replacement algorithms aim to minimise page-fault rate. We evaluate the algorithm by running it on a particular string of memory references (reference string), counting the number of page faults

The page replacement algorithms we will be looking at are
- First-In-First-Out (FIFO) algorithm
- Optimal algorithm
- Least Recently Used (LRU) algorithm
- Second-Chance Algorithm (or Clock algorithm)

### First-In-First-Out

- The simplest algorithm
- Replace the page that has been loaded in the memory for the longest period of time


It might be common to think that the more frames we have, the less page faults will occur. However this is not always true
- This is called [Belady's anomaly](https://en.wikipedia.org/wiki/B%C3%A9l%C3%A1dy%27s_anomaly)
- Most commonly experienced in FIFO page replacement algorithm
- By increasing number of pages, this changes the order at which items are removed from the page table, hence can actually increase the number of page faults

### Optimal Algorithm

- Replace the page that will not be used for the longest period of time
- Theoretically best, however infeasible because the OS has no way to know whether processes are used 
- Used as a benchmark to measure how well other algorithms perform
- Does not suffer from Belady's anomaly

### Least Recently Used (LRU) Algorithm

- Replace the page that has not been used for the longest period of time
- Similar to optimal algorithm, however looking backward in time (and not susceptible to Belady's anomaly)
- There are several implementations of LRU
    - Counter implementation
        - Each page table entry has a time-of-use field
        - The CPU increments a logical clock for every memory reference
        - Whenever a page is referenced, the value of the clock is copied to the time-of-use field of the page
        - When a page fault occurs, the page with the smallest time-of-use (the least recently used) value is kicked out
    - Stack implementation
        - ![Stack implementation of LRU](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter9/9_16_StackUse.jpg)
        - Keep a stack of page numberse in a doubly linked list
        - When a page is referenced, move it to the top
        - THe bottom of the stack is the LRU page
        - List update is expensive, but no search is needed for page replacement
- Hardware support necessary for exact algorithms that are expensive and generally not available
- However, many systems support a hardware settable reference bit
    - With each page associate a bit R, initially R = 0
    - When a page is referenced, hardware sets its R = 1
    - Replace page with R = 0 (if one exists). Note that the exact order of use cannot be determined by R
    - There can be many pages with R = 0
- Approximation algorithm: Second-Chance (clock) algorithm

## Second-Chance Algorithm

![Second chance implementation](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter9/9_17_SecondChance.jpg)

- A variation of FIFO
- Choose the oldest page as a candidate to be replaced
- If the reference bit R of the candidate is R = 1, then the page is given a second chance
    - The R bit is cleared, and the page is treated as if it has just been read in
    - Otherwise the page is kicked out

# Allocation of Frames

- The OS must decide how many pages to bring in to physical memory
    - The smaller the amount of memory allocated to each process, the more processes can reside in memory
    - However, small number of pages loaded increases page faults
    - Beyond a certain size, further allocations of pages will not affect the page fault rate

## Fixed vs Variable Allocation

Fixed Allocation
- Gives a pprocess a fixed number of frames in physical memory within which to execute
    - E.g. equal allocation or proportional allocation
- When a page fault occurs, one of the pages of that process must be replaced

Variable Allocation
- Allows the number of page frames allocted to a process to be varied over the lifetime of the process

## Global vs Local Allocation

Global replacement
- Process selects a replacement frame from the set of all frames
- One process can take a frame from another process
- Performance depends on external circumstances (other processes)

Local replacement
- Each process selects only from its own set of allocated frame
- May hinder other processes by not making available its less used pages/frames

| \                   | Local Replacement                                                                                                                           | Global Replacement                                                                                                                    |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| Fixed Allocation    | Number of frames allocated to a process is fixed, pages to be replaced is chosen from among the frames allocated to the process             | Not possible                                                                                                                          |
| Variable Allocation | Number of frames allocated to a process can change over time, page to be replaced is chosen from among the frames allocated to that process | Page to be replaced is chosen from all available frames in main memory, this causes the size of the resident set of processes to vary |

# Thrashing

![](https://s3.ap-south-1.amazonaws.com/s3.studytonight.com/tutorials/uploads/pictures/1610973226-71449.png)

A process that spends more time paging than executing is said to be thrashing.

- If a process does not have enough pages, the page-fault rate is very high
- This causes low CPU utilisation, because time is wasted paging in/out rather than actually executing the process
- A common wisdom to improve CPU utilisation is to increase the degree of multiprogramming
    - Another process is added to the system
    - CPU utilisation drops further

## Strategies to combat Thrashing

To prevent thrashing, we must provide processes with as many frames as they really need "right now". There are 2 strategies to do this

1. Working-Set model
2. Page fault frequency

### Working-Set model

- Locality of reference: Processes tend to refer to pages in a localised manner
    - Temporal locality: Locations referred to just before are likely to be referred to again (i.e. location references are clustered over time)
    - Spatial locality: Code and data are usually clustered physically
- This observation led to the Working-Set model

```c
int sum = 0;
for (int i = 0; i < n; i++) {
    sum += a[i];
}

return sum;
```

- Data
    - Temporal: `sum` referenced in each iteration
    - Spatial: elements of `a` are accessed in consecutive manner
- Instructions
    - Temporal: Cycle through loop repeatedly
    - Spatial: Reference instructions in sequence

- During the lifetime of a process, references are confined to a subset of pages
- Let $\Delta$ be the working-set window, which is a fixed number of page references
- For example, if $\Delta = 10$ memory references, then the working set at time $t$ are the set of all pages referenced for $t$ to $t + \Delta$

```
2, 6, 1, 5, 7, 7, 7, 7, 5, 1

WS(t1) = { 1, 2, 5, 6, 7 } 
```

![Working set size over time](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter9/9_22_PageFaultRate.jpg)

- $WSS_i$ (working set size of process $P_i$) = total number of pages referenced in the most recent $\Delta$ (varies in time)
- $D = \sum WSS_i$ = total demand for frames
- $m$ = total number of frames
- If $D > m$, thrashing occurred
- Policy: If $D > m$, then swap out one of the processes
- We want to fine $\Delta$ such that trashing is minimised.
    - If it is too small, it does not encompass all of the pages of the current locality
    - If it is too large, then it encompasses pages that are no longer being frequently accessed

### Page Fault Frequency Scheme

![Page fault frequency scheme](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter9/9_21_PageFaultFrequency.jpg)

- A more direct approach is to measure the page fault rate, and increase/decrease the number of frames depending on the page fault rate
- If the page fault rate is higher than the upper bound, we increase the number of frames allocated to a process
- If the page fault rate is lower than the lower bound, we decrease the number of frames allocated to a process
- May have to suspend (swap out) a process if the page fault rate increases, and no free frames are available

# Summary

Virtual memory
- Allows a large logical address space to be mapped to a smaller physical memory
- Demand paging is a way to implement virtual memory

Page replacement algorithms
- FIFO
- Optimal
- LRU
- LRU Approximation (Clock/Second-chance)

Frame Allocation
- Fixed vs Dynamic allocation
- Local vs Global allocation

Trashing: Process busy paging in/out, rather than executing
- Working-set model
- Page-fault frequency scheme

# Resources

- https://en.wikipedia.org/wiki/Demand_paging
- https://en.wikipedia.org/wiki/B%C3%A9l%C3%A1dy%27s_anomaly
- https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/9_VirtualMemory.html
# Memory Organisation

We need to find a way to bind code and data to memory
- Address binding, logical vs physical address space

Contiguous allocation
- Fixed vs dynamic partitioning, fragmentation

Paging
- Address translation, page table implementation, shared pages, 2-level page table, inverted page table

Segmentation
- Address translation

# Binding Code and Data to Memory

To run a program, a process image must be created and loaded into memory

![](https://media.geeksforgeeks.org/wp-content/uploads/memoryLayoutC.jpg)

Each program has the following

- Text: Where the code of the program is stored
- Data: Where the global parameters for the program is stored
- Heap: Where dynamically allocated variables are stored
- Stack: Where parameters and local variables in a function are stored

A compiler is used to translate source code to object code. A linker is used to combine object modules to resolve references. A loader is used to create the process image, allocate memory for the process, and load process image into the allocated memory

![](https://media.geeksforgeeks.org/wp-content/uploads/20200808221828/llgfg.png)

Address binding of instructions and data to memory addresses can happen in 3 different stages

1. Compile time: If memory location known a priori, absolute code can be generated (using absolute addresses); must recompile if starting location changes
2. Load time: Compiler generates relocatable code (using addresses relative to the start register), and binding is performed by the loader
3. Execution time: If the process can be moved during its execution from one memory segment to another, binding is delayed until runtime

In compile-time or load-time binding, **absolute address format** is used in process image
- Binding of logical address space to the physical memory is static
- Once process image is loaded, it cannot be moved in memory
- If address generated by CPU < base OR >= base + limit, then an error is thrown
- Otherwise, memory address = address generated by the CPU

However in execution-time binding, since we only know the address of the program in memory at execution time, we use **relative address format**
- Addresses tell you how far relative to the start register to go to
- If address generated by CPU >= limit, an error is thrown
- Otherwise, memory address = address generated by CPU + base register address

# Logical vs Physical Address Space

- Address space: All addresses accessible by a process
    - The range of addresses along a street
- Logical address: Addresses used in the code, generated by the CPU when executing an instruction
    - What you see when you want to navigate to a house, can be "5th house from the start" (relative), or "Block 1234" (absolute)
- Physical address: Address used to access physical memory, seen by the memory unit
    - The actual address of the house itself

# Allocating Memory Among Processes

![](https://media.geeksforgeeks.org/wp-content/uploads/20200405213343/Picture12.jpg)
![](https://media.geeksforgeeks.org/wp-content/uploads/20200405214155/Picture24.jpg)

There are 2 approaches to allocating memory

1. Contiguous Allocation: Logical address space of a process remains contiguous in physical memory
    - Fixed partitioning
    - Dynamic partitioning
2. Non-contiguous allocation: A process logical address space is scattered over different regions in physical memory
    - Paging

## Contiguous Allocation

In contiguous allocation, logical address space of a process maps to a contiguous space in physical memory (the memory remains together, unscattered)

### Fixed Partitioning

Memory is partitioned into regions with fixed boundaries before executing the program. When the program requires memory, it takes memory from a partition. 

- The number of partitions in memory is fixed
- The size of each partition may or may not be the same
- When memory is allocated, the entire partition is allocated to the code, regardless of how little of the partition was used
- No spanning is allowed
    - Spanning is when a single process spans across different spaces in main memory in non-consecutive order

Advantages of fixed partitioning
- Easy to implement
    - Simply requires putting a process into a certain partition without focusing on the emergence of internal or external fragmentation
- Little OS overhead
    - Requires lesser excess and indirect computational power

Disadvantages of fixed partitioning
- Internal fragmentation
    - Main memory use is inefficient. Any program, regardless of size, occupies an entire partition. This can cause internal fragmentation
- External fragmentation
    - Total unused space of various partitions cannot be used to load the processes, even though there is space available, but not in contiguous form, since spanning is not allowed

### Dynamic Partitioning

When a process arrives, it is allocated memory from a hole large enough to accomodate it
- A hole is a block of available memory; holes of various sizes are scattered throughout memory
- OS maintains information about
    - Allocated partitions
    - Free partitions (holes)

There are multiple ways to allocate storage when using dynamic partitioning

- First-fit
    - Allocate the first hole that is big enough for the process
- Best-fit
    - Allocate the smallest hole that is big enough
    - Must search the entire list first, unless ordered by size
    - Produces smallest leftover hole
- Worst-fit
    - Allocate the largest hole
    - Must search entire list first, unless ordered by size
    - Produces largest leftover hole

Advantages of Dynamic partitioning
- No internal fragmentation
    - Given the fact that partitions in dynamic partitioning are created according to the needs of the process, there will not be any internal fragmentation due to not being any unused space in the partition
- No limitation on process size
    - For fixed partitioning, processes must be smaller than the largest partition, or else they will not be able to execute. In dynamic partitioning, process size is not restricted because partition size is determined according to process size
- Degree of multiprogramming is dynamic
    - Due to absence of internal fragmentation, there will not be unused space in the partition hence more processes can be loaded in the memory at the same time

Disadvantages of dynamic partitioning
- External fragmentation
    - Dynamic partition just allocates memory to processes directly without worrying about how the processes are stored in relation to one another. Hence external fragmentation can still occur
- Complex memory allocation
    - Allocation and deallocation of memory is complex since partition sizes will always keep changing. OS has to keep track of all partitions

#### Fragmentation

There are 2 types of fragmentation

![Internal vs External Fragmentation](https://i.stack.imgur.com/dSWgj.gif)

1. External fragmentation
    - There is enough total memory space to satisfy a request, however it is not contiguous. Happens outside a partition
2. Internal fragmentation
    - Allocated memory may be slightly larger than requested memory
    - This size difference is memory internal to a partition, but not being used

We can reduce external fragmentation by compaction
- Shuffle memory contents to place free memory together into one large block
- Compaction is possible only if relocatable address format is used in process image, and binding is done during execution time

![Compaction](https://exploringbits.com/wp-content/uploads/2021/01/Compaction-in-operating-system.png)

## Non-Contiguous Allocation

In non-contiguous allocation, even if the logical addresses are consecutive, that does not mean that the physical memories that those logical addresses are mapped to are consecutive as well.

### Paging

![Paging](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter9/9_01_VirtualMemoryLarger.jpg)

- Physical memory is allocated to a process whenever physical memory is available
- In paging, physical memory is divided into fixed-sized blocks called **frames** (size is a power of 2, usually between 512 and 8192 bytes)
- Divide logical memory into blocks of the same size called **pages**
- OS keeps track of all free (unallocated) frames
- To run a program of size n pages, need to find n free frames, and load the program 
- Set up a **page table** to translate logical to physical addresses
- This eliminates external fragmentation
- Internal fragmentation is still possible because processes may not be as big as a page

#### Address Translation Scheme

Logical address contains

1. Page number (p) - used as an index into a page table entry which contains the frame number in physical memory
2. Page offset (d) - combined with frame number to define the physical memory address that is sent to the memory unit

E.g.

CPU generates a logical address 1011 0010, where each page is $2^4 = 16$ bytes. 
1. The logical address is split into 2 parts
    - Page index: 1011
    - Offset: 0010
    - Since each page is $2^4$ bytes, there are $2^4$ possible addresses on a single page. Hence the offset is $4$ bits 
    - The page index is represented by the remainder of the bits in the logical address
2. The page index is used to check the page table, and see which address in physical memory the logical address was mapped to
    - Since the page index has 4 bits, there are actually $2^4 = 16$ page entries
3. The physical address is concatenated with the offset to find the overall address in physical memory to look for the information required

#### Implementation of Page Table

- Page table is kept in physical memory
- Page-table base register (PTBR) points to the page table (for each process)
- Page-table length register (PTLR) indicates the size of the page table
- In this scheme, every data/instruction access requires 2 memory accesses: one for page table, one for the data/instruction
    - Effective memory access time = $w \mu$, where each memory cycle access time is $\mu$ time units
- Memory access time can be reduced by the use of a special fast-lookup hardware cache called **associative registers** or **translation look-aside buffers** (TLBs)

![Page table with TLB](https://media.geeksforgeeks.org/wp-content/uploads/20190225192626/tlb1.jpg)

1. CPU generates logical address to retrieve data from
2. Check TLB to see if entry is within TLB
    1. If page is in TLB, we can immediately load the physical address
    2. If page is not in TLB, we hit the page table
    3. We can now load the physical address, and update the TLB with the new entry

#### Effective Access Time

Effective access time is the average time it takes to get a value from memory.

Consider that the time to lookup the TLB is $\epsilon$ time units, while a memory cycle time is $\mu$ units. The hit ratio ($\alpha$) is defined as the percentage of times that a page number is found in the associative registers

The effective access time (EAT) is

$$
(\mu + \epsilon) \alpha + (2 \mu + \epsilon) (1 - \alpha) = (2 - \alpha) \mu + \epsilon
$$

For $\alpha$ of the time, we only hit the TLB and retrieve from memory, hence $(\mu + \epsilon) \alpha$. For the remaining $1-\alpha$ times, we hit the TLB ($\epsilon$), and then have to hit the page table ($\mu$) which is in the physical memory, and then we retrieve the data ($\mu$). Hence $(2 \mu + \epsilon)(1-\alpha)$.

#### Shared Pages



# Resources

- https://www.geeksforgeeks.org/fixed-or-static-partitioning-in-operating-system/
- https://stackoverflow.com/questions/1200694/internal-and-external-fragmentation
# Density Estimation

Purpose of machine learning

- Model uncertainty (Density estimation)
- We want to figure out shape of probability distribution

Recall Naive Bayes

$$
\begin{aligned}
y_i &= \argmax_c P(\bold{x} | y = c) P(y = c) \\
&= \argmax_c \prod_{i=1}^{d} P(x_i | y = c) P(y = c)
\end{aligned}
$$

# Discrete Probability Distributions

- Usually finite number of outcomes
- 6-sided die: 6 possible outcomes
- Distribution can be described with 6 numbers
  - Non-negative
  - Sums to 1

# Continuous Probability Distributions

- Outcome is a real number in a range (e.g. $[0, 1], (-\infty, \infty)$)
- Infinite number of outcomes between any 2 real numbers

# Probability Density Function

Probability is area under the curve

$$
\begin{aligned}
    y &= p(x) \\
    \int_{-\infty}^{\infty} p(x) dx &= 1 \\
    P(\bold{x}^*) &= \int_{\bold{x}^*}^{\bold{x}^* + \epsilon} p(x) dx \\
    &\approx p(\bold{x}^*) \times \epsilon
\end{aligned}
$$

# Density Estimation

Density estimation aims to estimate an unobservable underlying probabilty density function, based on observed data

- Denote by $\mathcal{D} = \{ \bold{x}_1, ..., \bold{x}_N \}$ the set of observed data points, drawn from an unknown probability distribution $P(\bold{x})$

  $$
      \bold{x}_i \sim P(\bold{x}), \ i = 1, ..., N
  $$

- Goal is to estimate the associated probability density function $p(\bold{x})$ of the distribution

# State-of-the-Art Density Estimation

Generative Adversarial Networks

- Competition drives the counterfeiter to learn the distribution of real data

Diffusion models

- Learn a sequence of transformations that create images from pure noise
- E.g. DALL-E V2, 2022, sampling from conditional distribution $P(image | text)$

# Density Estimation Approaches

- Parametric density estimation
  - Assume a form for $p(\bold{x}; \bold{\theta})$, defined up to parameters, $\theta$
  - E.g. Gaussian distribution $\mathcal{N}(\mu, \sigma^2), \bold{\theta} = \{ \mu, \sigma^2 \}$
  - Estimate $\bold{\theta}$ from observed data points (Maximum likelihood estimation)
- Nonparametric density estimation

# General Principle

Observed data points $\mathcal{D} = \{ \bold{x}_1, ..., \bold{x}_N \}$ are assumed to be a sample of $N$ random variables, independent and identically distributed (iid)

- Identically distributed: for any $\bold{x}_i \in \mathcal{D}$, it is sampled from the same probability distribution
- Independent: All data points $\bold{x}_i \in \mathcal{D}$ are independent events

# Parametric Density Estimation

Assume that $\mathcal{D} = \{ \bold{x}_1, ..., \bold{x}_N \}$ are drawn from some known probability density family $p(\bold{x} | \bold{\theta})$, defined up to parameters $\bold{\theta}$

- We seek $\bold{\theta}$ that makes $\bold{x}_i$ as likely as possible under $p(\bold{x} | \bold{\theta})$
- Approach: Maximum likelihood estimation

# Maximum Likelihood Estimation

The likelihood of sample $\mathcal{D}$ given parameter $\bold{\theta}$: $\mathcal{L}(\mathcal{D}; \bold{\theta})$

- As $\mathcal{D}$ is IID, the above likelihood is the product of the likelihood of the individual points

$$
    \mathcal{L}(\mathcal{D}; \bold{\theta}) \triangleq P(\mathcal{D} | \bold{\theta}) = \prod_{i=1}^{N} P(\bold{x}_i | \bold{\theta}) \propto \prod_{i=1}^{N} p(\bold{x}_i | \bold{\theta})
$$

In MLE, we aim to find $\bold{\theta}$ that makes $\mathcal{D}$ the most likely to be drawn from. Mathematically, we aim to search for $\hat{\bold{\theta}}$ such that

$$
\bold{\hat{\theta}} = \argmax_\theta \mathcal{L}(\mathcal{D}; \bold{\theta})
$$

# Log-Likelihood

Typically, we maximise the log-likelihood

$$
\mathcal{L}(\mathcal{D}; \bold{\theta}) \triangleq \ln \mathcal{L}(\mathcal{D}; \bold{\theta})
$$

- $\ln$ function converts product into sum

$$
\ln \mathcal{L}(\mathcal{D}; \bold{\theta}) = \ln \left( \prod_{i=1}^{N} p(\bold{x}_i | \bold{\theta}) \right) = \sum_{i=1}^{N} \ln p(\bold{x}_i | \bold{\theta})
$$

- $\ln$ is also a strictly increasing function, hence one can maximise the log likelihood without changing the value where it can take its maximum

$$
\bold{\hat{\theta}} = \argmax_\bold{\theta} \mathcal{L}(\mathcal{D}; \bold{\theta}) = \argmax_\bold{\theta} \ln \mathcal{L}(\mathcal{D}; \bold{\theta})
$$

# Solving MLE

Suppose $\bold{\theta} \in \mathbb{R}^p$ contains $p$ parameters

$$
\bold{\theta} = [\theta_1, ..., \theta_p]^T
$$

Then, $\argmax_\bold{\theta} \ln \mathcal{L}(\mathcal{D}; \bold{\theta})$ is an unconstrained optimisation problem

- To solve it, we first set the derivative of $\ln \mathcal{L}(\mathcal{D}; \bold{\theta})$ wrt $\bold{\theta}$

$$
\nabla_\bold{\theta} \ln \mathcal{L}(\mathcal{D}; \bold{\theta}) = \nabla_\bold{\theta} \left( \sum_{i=1}^{N} \ln p(\bold{x}_i | \bold{\theta}) \right) = \sum_{i=1}^{N} \nabla_\bold{\theta} \ln p(\bold{x}_i | \bold{\theta}) = 0
$$

- Recall $\nabla_\bold{\theta} = \left[ \frac{\partial}{\partial \theta_1}, \cdots, \frac{\partial}{\partial \theta_P}\right]$
- We obtain a solution for $\hat{\bold{\theta}}$ by solving the above system of equations

# Univariate Gaussian

Suppose $\mathcal{D} = \{x_1, ..., x_N\}$. Each data point $x_i$ is a scalar, and is drawn from a Gaussian distribution with unknown $\mu$ and $\sigma^2$

$$
p(x | \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{(x - \mu)^2}{2 \sigma^2}}
$$

where

$$
\begin{aligned}
    \mu &= \mathbb{E}[x] \\
    &= \int p(x; \mu, \sigma^2) x \ dx \\
    \sigma^2 &= Var(x) \\
    &= \mathbb{E}[(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T]
\end{aligned}
$$

Log-likelihood is:

$$
\begin{aligned}
    \ln \mathcal{L}(\mathcal{D}; \bold{\theta}) &= \sum_{i=1}^{N} \ln p(\bold{x}_i | \bold{\theta}) \\
    &= \sum_{i=1}^{N} \ln \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left({- \frac{(x - \mu)^2}{2 \sigma^2}} \right) \right) \\
    &= \sum_{i=1}^{N} \ln \left( \exp \left( -\frac{(x_i - \mu)^2}{2 \sigma^2} \right) \right) - \sum_{i=1}^{N} \ln \sqrt{2 \pi \sigma^2} \\
    &= - \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{2 \sigma^2} - \frac{N}{2} \ln (2\pi) - \frac{N}{2} \ln \sigma^2
\end{aligned}
$$

Now we differentiate:

$$
\begin{aligned}
    \nabla_\bold{\theta} \ln \mathcal{L}(\mathcal{D}; \bold{\theta}) &=  \sum_{i=1}^{N} \nabla_\bold{\theta} \ln p(x_i | \bold{\theta}) \\
    &= \begin{bmatrix}
        \sum_{i=1}^{N} \nabla_\mu \ln p(x_i | \bold{\theta}) \\
        \sum_{i=1}^{N} \nabla_{\sigma^2} \ln p(x_i | \bold{\theta})
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \frac{1}{\sigma^2} \sum_{i=1}^{N} (x_i - \mu) \\
        \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{2 (\sigma^2)^2} - \frac{N}{2 \sigma^2}
    \end{bmatrix}
\end{aligned}
$$

Now we set the derivatives to be 0

$$
\begin{cases}
    \frac{1}{\sigma^2} \sum_{i=1}^{N} (x_i - \mu) = 0 \\
    \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{2 (\sigma^2)^2} - \frac{N}{2 \sigma^2} = 0
\end{cases}
$$

Solving,

$$
\begin{cases}
    \hat{\mu} &= \frac{1}{N} \sum_{i=1}^{N} x_i \\
    \hat{\sigma^2} &= \frac{1}{N} \sum_{i=1}^{N} (x_i - \hat{\mu})^2
\end{cases}
$$

# Unbiased Estimator

An estimator is a rule for estimating a quantity based on observations

- MLE estimator for gaussian mean is $\hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} x_i$
- Estimator is unbiased if its expectation is the same as the true quantity

$$
\mathbb{E}[\hat{\mu}] = \mu \\
$$

However $\hat{\sigma}^2$ is biased:

$$
\mathbb{E}[\hat{\sigma}^2] = \frac{N - 1}{N} \sigma^2
$$

Note [this](https://www.probabilitycourse.com/chapter8/8_2_2_point_estimators_for_mean_and_var.php#:~:text=Although%20the%20sample%20standard%20deviation,a%20biased%20estimator%20of%20%CF%83.) and [this](https://math.stackexchange.com/questions/149723/why-is-the-expected-value-ex2-neq-ex2)

To correct bias,

$$
\tilde{\sigma}^2 = \frac{N}{N-1} \hat{\sigma}^2 = \frac{1}{N-1} \sum_{i=1}^{N} (x_i - \hat{\mu})^2
$$

# Multivariate Gaussian

Slide 30

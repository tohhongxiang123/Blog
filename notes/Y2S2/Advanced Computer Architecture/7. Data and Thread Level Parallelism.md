# Data and Thread Level Parallelism

There are 3 levels of parallelism

1. Instruction level parallelism
    - Multiple independent instructions are identified and grouped to be executed concurrently in different functional units in a single processor
    - Can reduce CPI to less than 1
    - E.g. Superscalar and VLIW processors
2. Data level parallelism
    - Same operation is performed on multiple data values concurrently in multiple processing units
    - Can reduce the instruction count to enhance performance
    - E.g. vector processors, array processors
3. Thread/Task level parallelism
    - More than 1 independent threads/tasks are executed simultaneously
    - Can reduce the total execution time of multiple tasks
    - E.g. multi core and multi processor systems

## Data-Level Parallelism

-   E.g. convert all characters in an arra to uppercase
    -   Can divide parts of the data between different tasks, and perform the tasks in parallel
    -   Key: No dependencies between tasks that cause their results to be ordered
-   Same set of operations are performed on different data elements
    -   E.g. adding 2 100-dimensional vectors
-   Such data parallel computation can be performed for matrix product, matrix addition, matrix-vector product, vector addition, and dot-product of vectors etc
-   Matrix computations are widely used in scientific computing, database operations, image, audio and video processing applications
-   Parallel computing systems need to be used for efficient use of data-level parallelism

# Computing Models-Flynn's Classification

-   Single instruction, single data stream (SISD)
    -   Conventional sequential processor
    -   Not suitable to realise data-level parallelism
-   Single instruction, multiple data stream (SIMD)
    -   Efficiently utilise the data-level parallelism
-   Multiple instruction, single data stream
    -   No commercial programmable system
-   Multiple instruction, multiple data stream (MIMD)
    -   True multiprocessor system that issues multiple instructions
    -   Can support data level parallelism

![SISD architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/SISD.svg/1200px-SISD.svg.png)
![SIMD architecture](https://en.algorithmica.org/hpc/simd/img/simd.png)
![MISD architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/MISD.svg/1200px-MISD.svg.png)
![MIMD architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/MIMD.svg/1200px-MIMD.svg.png)

## SIMD

-   In each cycle, only 1 machine instruction is issued
-   The same instruction is executed for different data elements by different processors
-   Central control unit issues instructions and controls simultaneous execution of instructions
-   Each processing element has its own data memory and also could interact via shared memory
-   All processing elements work synchronously
-   Applications: Signal processing, matrix/scientific computation

## Advantages of SIMD over MIMD

-   SIMD architecture make use of data level parallelism efficiently. If there are $N$ processing units, $N$ operations can be performed in 1 clock cycle (high performance)
-   Several operations on different data can be realised by one instruction: The instruction bandwidth (i.e. the number of instructions to be used for a task) is reduced by a factor on $N$
-   Loop-overhead instructions (address-increment and branch condition checks) will also be reduced by a factor of $N$

    -   E.g.

    ```c
    int x;
    for (x = 0; x < 100; x++)
    {
        delete(x);
    }

    // unrolled loop

    int x;
    for (x = 0; x < 100; x += 5 )
    {
        delete(x);
        delete(x + 1);
        delete(x + 2);
        delete(x + 3);
        delete(x + 4);
    }
    ```

    -   Note how there are less branch condition checks, and less address-incrementing

-   Energy used in instruction fetch will be reduced. SIMD is more energy-efficient than MIMD. It makes SIMD attractive for personal mobile devices
-   Biggest advantage of SIMD versus MIMD is that the programmer continues to think sequentially, yet achieves parallel speedup by having parallel data operations

# Types of SIMD Computing Systems

-   Vector processors
    -   Pipelined execution of many data operations
    -   Operations on multiple data elements are performed in consecutive time steps (clock cycles) in pipelined form
-   SIMD array processors
    -   Operations are performed on multiple data elements at the same time by multiple processing elements
-   Multimedia SIMD instruction set extensions
    -   MMX (Multimedia extensions)
    -   SSE (Streaming SIMD extensions)
    -   AVX (Advanced vector extensions)

## Vector Processor

-   A kind of SIMD processor
-   Instructions of a vector processor operate on 1D arrays of data called vectors
-   Arrays are primitive data elements. Need to perform multiple arithmetic and logic operations in parallel. Consists of multiple functional units. Vector-registers and heavily-interleaved memory
-   Typical vector operations
    -   Add 2 vectors
    -   Subtract 2 vectors
    -   Multiply 2 vectors
    -   Divide 2 vectors
    -   Load a vactor from memory
    -   Store a vector to memory

### Properties of Vector Processor

-   Multiple parallel operations: One vector instruction can perform N computations, where N is the vector length
    -   Involves less number of instructions
    -   Lower instruction count -> Less execution time -> higher performance
-   Computation in a given clock cycle is independent of previous results: Dependence check is not required -> Simpler design
    -   Allows deep pipeline of functional unit -> High clock rate
-   Fewer branches: Less branch overhead instructions
-   Memory access pattern per vector instruction known
    -   Parallel memory access helps reduce memory latency
    -   Can use a high-bandwidth memory system
    -   Data caches need not be used

### Classes of Vector Processors

-   Memory-memory vector processors
    -   Vector operands are fetched from memory, results are also stored in the memory
-   Vector-register processors: Vector equivalent of load-store architectures
    -   Except load and store instructions, for all instructions, operations are all only between registers
    -   Memory operands are made available in the registers, and then operations are performed on register operands

![Array vs Vector processors](https://i.ytimg.com/vi/pB5-6RhSnmw/maxresdefault.jpg)

# SIMD Instruction Set Extensions for Multimedia

-   Multimedia applications are popular and we need an instruction set architecture to match that properly
    -   Media applications exhibit high data-level parallelism
    -   Each instruction can operate on a group of data eleemnts to reduce instruction bandwidth
-   Most media applications operate on narrower data types
    -   Many graphics systems use 8 bits to represent each of the 3 primary colors, plus 8 bits for transparency
    -   Image pixels are 8bit integer, audio samples are usually represented with 8 or 16 bits, while bit-width of the processor is usually 32 or 64 bits
-   Storing 1 data element in a word inefficiently utilises the processor resources
    -   Can pack 4 or 8 data elements in a 32-bit or 64 bit word respectively?
    -   Can read 4 elements of an array with 1 32-bit load, and similarly for stores?

# SIMD Extensions

-   Data elements are packed in a word like short vectors
-   Up to 8 operations can be performed in parallel in a 64 bit processor
    -   Potential performance enhancement for image processing applications is 8x
    -   In practice performance improvement is less that 8x, but still good

# MMX Instruction Set

-   57 new instructions comprised of
    -   Integer arithmetic operations such as packed add, sub, multiply and multiply-add, packed logical operations
    -   Shift and rotate, compare
    -   Move: From/to memory and from/to registers
    -   Pack/unpack (Conversion between packed data types)
-   Like vector instructions, an SIMD instruction specifies the vector operations on vectors of data
-   SIMD extension instructions specify fewer operands, henc use smaller register files compared to vector procesors with large register files like VMIPS
-   Does not require much modification over existing architecture
    -   Small, additional cost to enhance the standard ALU and easy to implement: Increase in implementation complexity is marginal
    -   Co-exists with existing processor to implement general applications efficiently

# Thread Level Parllelism

# Motivation for Multicore

-   Need for performance enhancement
    -   Need a faster, more capable system to support increasing transmission speeds and communication bandwidth
    -   Increasing resolution of image/video data
    -   Increasing security need
    -   Increasing use of video conferencing and surveillance
    -   New applications which require enhanced performance such as drug discovery with high volume DNA data processing, scientific computing, etc.

# Availability of Computing Resources: Moore's Law

-   Moore's law states that the number of transistors in an IC chip double approximately every 18 months

# Traditional Approaches for Performance Enhancement

-   Performance is the reciprocal of execution time
-   2 traditional approaches to boost CPU performance
    1. Increase clock speed
    2. Decrease clocks per instructions by instruction level parallelism

# Instruction Level Parallelism

-   Pipelined instruction execution
-   Multiple instructions issued per cycle: Superscalar and VLIW (Very long instruction word) processors
-   Branch prediction, speculative execution, out-of-order execution
-   Advanced cache design

# ILP Wall

-   High design and verification time and cost
-   Diminishing returns in more ILP
-   Instruction-level parallelism is near its limit

# Power Wall

-   $P = ACV^2f$
-   Power consumption increases more rapidly with the increasing operating frequency
-   Around the beginning of 2003, the ever-increasing procesor speed was checked due to extremely high power consumption
-   Intel Tejas dissipated 150W of heat at 2.8 GHz. It was projected to run at 7 GHz.

# Memory Wall

-   Widening gap between compute bandwidth and memory bandwidth could not be bridged - resulting in memory latency
-   Memory hierarchy was proposed as the solution
-   However, increase in size of the on-chip cache and cache optimisation no longer yield improvement

# Brick Wall

-   Power wall + ILP wall + memory wall = brick wall
-   It could be possible to overcome the brick wall with a multicore processor
-   A multicore processor contains 2 or more processors in the same chip, that provides enhanced performance by efficient simultaneous processing of multiple tasks and consumes less power due to lower clock frequency

# Need for Multicore

-   Task parallelism: Several functions on the same data: Average, minimum, binary, geometric mean
-   No dependencies between tasks, so all can run in parallel
-   Overcome power wall using multiple slower cores
    -   Cores running at a lower clock frequency and lower voltage can still deliver the desired performance using less power
    -   Scale up the number of cores rather than the frequency
-   Overcome memory wall with memory parallelism
-   Derive parallelism by thread-level parallism
-   Homogenous multicore system: Consists of identical cores: Less design and verification costs
-   Heterogeneous multicore systems: Contains different types of cores, each optimised for a different role

# Applications of Multicore

-   Database servers
-   Web servers
-   Telecommunication markets
-   Multimedia applications
-   Scientific applications
-   Almost anything that can be threaded today will map efficiently to multi-core. However, some applications remain difficult to parallelise

# Challenges: A New Power Wall

-   Technology scaling continues according to Moore's law: Increasing transistor density -> More heat dissipation per unit area
-   New power wall will constrain multicore processors too

-   Increase in memory latency is coming as a new memory wall
-   As number of cores increase, the performance enhancement is degraded
    -   Due to lack of memory bandwidth
    -   Contention between cores over memory bus

# Challenges: Interconnect Problem

-   On-chip interconnects are becoming a critical bottleneck
-   Increasing number of cores, interconnect length increases since data moves across the cores on the chip
-   Interconnect delay increases with technology: would be reaching above 12ns for 1mm of copper wire by 2020
-   Interconnect power density has been rapidly incr3easing with feature size
-   Also requires mechanisms for efficient inter-processor coordination
    -   Synchronisation
    -   Mutual exclusion
    -   Context switching

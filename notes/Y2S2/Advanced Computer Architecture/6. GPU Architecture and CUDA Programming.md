# GPU Architecture and CUDA Programming

- GPUs (Graphical Processing Unit) were designed to accelerate 3D graphics renderings on PC, such as for 3D games
- However, GPUs have now also been used extensively for mathematical and scientific computing purposes, such as AI applications
- One particular area that AI has been used extensively for is computer vision
  - Using machines to classify images, cluster similar images, perform object recognition etc.

# Convolutional Neural Networks (CNN) in AI

CNN is used extensively in many computer vision applications

- CNN based neural network techniques havfe been at the heart of spectacular advances in deep learning
- Particularly suitable to be implemented with GPUs
- CNNs involve massive amounts of convolutions/correlations
  - Hence, the effectiveness of a CNN depends on efficient implementation of matrix operations, mainly consisting of multiplication and addtion/accumulation (MAC) operations
  - Applying to independent sets of data, hence can be performed in parallel
  - Efficient if we have suitable processor architectures that support massive SIMD operations
    - SIMD is short for Single Instruction/Multiple Data, while the term SIMD operations refers to a computing method that enables processing of multiple data with a single instruction

# Recap: Data Level Parallelism

- Same operation is performed on multiple data values
  - Can be executed concurrently with multiple processing units
  - Single instruction multi data - SIMD
- SIMD execution
  - Vector processor
  - Array processor
  - SSE, AVX for multimedia support on modern CPUs
  - GPU

# Basic CPU Architecure: Single Core

- Designed for single-threaded code optimised for low latency
  - By using various performance enhancement techniques
  - Through sophisticated processor logic circuitry

# MultiCore CPU Architecture

Adding more (and simpler) cores

- Able to execute multiple instructions in parallel
- Effective if we can write efficient concurrent code (e.g. Pthread based multithreaded programming)

# CPU Architecture with SIMD ALUs

To better suport SIMD operations

- Duplicate ALU within each core
- All ALUs within each core must execute the same instruction simultaneously

# Basic GPU Architecture

- Consists of many cores, each with many ALUs (Streaming processors (SP)) to support SIMT (Single instruction, multiple threads)
- Enables massive parallel MAC (floating point) operations

# Evolution of GPU Computing

- Before GPU,
  - Graphics display is based on a simple framebuffer subsystem standard known as VGA (Video graphics array)
  - Graphics stored as bitmap based images in the frame buffer

# Fixed Function Pipelines

Original design of GPU

- Based on fixed function pipelines
- Configurable, but not programmable
- Inflexible

Host interface receives graphics API commands and data from CPU

- E.g. through DMA transfer

# Programmable Graphics Pipeline

Certain functions executed at a few graphics pipeline stages vary with rendering algorithms

- Motivated the hardware designers to make those pipeline stages programmable
- E.g. vertex shader and pixel shader

# Processors based GPU

- Introduces programmable vertex and pixel processors

# Unified GPU Architecture

Instead of separate processors for each processing type, use the same processor core

# GPU Processor Internals

- Basic GPU Core
- Add more ALUs
- Add more independent blocks of ALUs

# NVIDIA GPUs

Modern GPUs are especially well-suited to address non-graphics problems that can be expressed as _data parallel_ computations with high arithmetic intensity

NVIDIA CUDA enabled GPU families

- CUDA programming platform
- Tesla (2008), Fermi, Kepler, Maxwell, Pascal, Volta, Turing (2018)

# Overview of NVIDIA GPU Architecture (Fermi)

![Nvidia Fermi Architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Fermi.svg/220px-Fermi.svg.png)

- Consists of 16 streaming multiprocessors
  - SMs are general purpose processors with a low clock rate target and a small cache.
  - The primary task of an SM is that it must execute several thread blocks in parallel.
  - As soon as one of its thread block has completed execution, it takes up the serially next thread block.
  - In general, SMs support instruction-level parallelism but not branch prediction.
- Each SM contains 32 compute engines known as CUDA cores
  - CUDA (Compute Unified Device Architecture), is a specialized programming language that can leverage the GPU in specific ways to perform tasks with greater performance.
  - CUDA Cores are parallel processors, just like your CPU might be a dual- or quad-core device, nVidia GPUs host several hundred or thousand cores.
  - The cores are responsible for processing all the data that is fed into and out of the GPU, performing game graphics calculations that are resolved visually to the end-user.
  - An example of something a CUDA core might do would include rendering scenery in-game, drawing character models, or resolving complex lighting and shading within an environment.
  - Each core has an integer ALU and FPU, which can execute a floating point/integer instruction per clock cycle
  - ![Fermi architecture with CUDA Core](https://miro.medium.com/max/1400/1*Wj6gB_MhhnmGu3OuToAjJg.jpeg)
- Each SM also has
  - 16 load/store units
  - 4 special function units (SFUs) to execute transecendental instructions (sin, cosine, reciprocal, square root)
  - 64KB of configurable shared memory and L1 cache

## Comparison with Nvidia Pascal GPU Architecture

- Total of 60 SMs with 3840 CUDA cores, arranged in the form of
  - 6 GPC (Graphic processing clusters) that each contain 10 SMs, which consists of 64 CUDA cores per SM

## Comparison with Nvidia Turing GPU Architecture

- Total of 72 SMs with 4608 CUDA cores (and 576 tensor cores for matrix operations)

# GPU Computing with CUDA Programming

- CUDA is a general purpose parallel computing programming model, where a problem is first expressed in the form of multiple threads
  - Each thread is executed in parallel using the CUDA cores in the GPU
  - Enables many complex computational problems to be solved in a very efficient way

In a CUDA program

- The problem is first decomposed into sub-problems
- Each of which is allocated to a "thread block"

Each sub problem is then separated into finer pieces that can be solved cooperatively in parllel using multiple threads within the thread block

# CUDA Programming Model

- CUDA operates on a heterogeneous programming model, consisting of a host and a device
  - Host is typically a CPU and its memory (host memory)
  - Device is the GPU and its memory (device memory)
- A program will start and is run by the host
  - The host launches the parallel threads which are executed on the device

# Basic CUDA C Program - Hello world

```c
int main(void) {
    printf("Hello, World!");
    return 0;
}
```

This is a simple CUDA C program, which only runs on the host, just consisting of standard C statements

- Program file is stored with the `.cu` extension (e.g. `hello_world.cu`)
- This `.cu` program file is compiled using the NVIDIA compiler `nvcc` (`nvcc hello_world.cu -o hello_world`)

## Device Code (Kernel)

To indicate the code that is to run on the device, we use the CUDA C keyword `__global__` declaration specifier

```c
__global__ void hello_GPU(void) {
    printf("Hello from GPU");
}
```

Device code are launced as kernel in CUDA, which can be called from the host code as follows

```c
int main(void) {
    hello_GPU<<<1, 1>>>();
    printf("Hello from CPU");
    return 0;
}
```

## Behind the Scenes

- During compilation, the source file is split into host and device components
- Device code is compiled by NVIDIA's compiler (`__global__` codes segments)
- Host code is compiled by using standard host compiler, such as GCC

## Kernel and Parallel Threads

- A kernel is an extended C function
  - It can be executed N times in parallel by N different CUDA threads when called (as opposed to only once for regular C functions)
- The number of parallel CUDA threads launched for the kernel is specified in thehost code using triple angle brackets syntax
  - `hello_dev<<<2, 4>>>();`
  - This is known as the execution configuration syntax
- The 2 parameters between the brackets specify how the threads are to be launched
  - Number of thread blocks (2 for above)
    - A thread block is a programming abstraction that represents a group of threads that can be executed serially or in parallel
    - For better processing and data mapping, threads are grouped into thread blocks
  - Number of threads in each block (4 for above)
  - `kernel<<<2, 4>>>();` launches 2 thread blocks, each with 4 threads
    - Total of 8 threads, executing 8 copies of the kernel
    - On the latest NVIDIA GPUs, a thread block may contain up to 1024 threads

## Thread ID

- How to identify different threads?

  - Each thread that executes a copy of the kernel is given a unique thread ID within the block
  - Thread ID is accessible within the kernel through the built-in `threadIdx` variable

  ```c
  __global__ void hello_GPU(void) {
      int i = threadIdx.x;
      printf("Hello from GPU[i]!\n");
  }
  ```

  - Output:

  ```
  Hello from GPU[0]!
  Hello from GPU[1]!
  Hello from GPU[2]!
  Hello from GPU[3]!
  ```

## Block ID

- With multiple thread blocks, each block executes the kernel is also given a unique block ID
  - accessible within the kernel through the built-in `blockIdx` variable
- Threads among different blocks can then be identified through the `threadIdx` and `blockIdx`, together with the built-in variable `blockDim` - which corresponds to the number of threads per block

  - Eg: Launching `hello_GPU<<<2, 4>>>()`

  ```c
  __global__ void hello_GPU(void){
      int i = blockIdx.x * blockDim.x + threadIdx.x;
      printf(“Hello from GPU[i]!\n”);
  }
  ```

## Multidimension Threads and Blocks

- Why are threads and blockIds extended with `.x`?
- CUDA threads and blocks can be defined to be of 1-dimensional, 2-dimensional (x and y) and 3-dimensional (x, y, z)
  - Multidimensional blocks are suitable for addressing complex proboelm best described in multi-dimensions (e.g. Matrix)

# Synchronisation between Host and Device

```c
__global__ void hello_GPU(void) {
    printf("Hello world from GPU\n");
}

int main(void) {
    hello_GPU<<<1, 4>>>();
    printf("Hello from CPU");
    return 0;
}
```

- The program above will most likely not work properly
  - The `printf` might not run in the order we want to, because the CUDA cores are asynchronous
- When we need the host to wait for all the threads to complete the kernels execution, use the CUDA function `cudaDeviceSynchronize()`

# Passing of Paremeters

We will look at performing vector addition

- In practice, we often need to pass parameters from the host to the device for computation
- We have to also collect the results back from the device to the host

Consider the addition of 2 vectors (a + b)

- Result is another vector c

## Vector addtion (Host only)

```c
int main(void){
    int N = 3;
    int a[N] = {7,2,3};
    int b[N] = {6,4,5};
    int c[N];
    vector_add(&c[0], &a[0], &b[0], N);
    return 0;
}

void vector_add(int *h_c, int *h_a, int *h_b, int n){
    for (int i = 0; i < n; i++)
        h_c[i] = h_a[i] + h_b[i];
}
```

## Memory Management between Host and Device

This vector addition is obviously most suitable to be executed by using the GPU

- We can launch `n` threads, for each of the entry in the vector
- But we have to first pass data from the host to the device (GPU)
  - This is done using the CUDA memory management functions: `cudaMalloc()`, `cudaMemcpy()`, and `cudaFree()`
- Similar to the standard C equivalent functions `malloc()`, `memcpy()`, and `free()`

```c
int main(void){
    int N = 3
    int a[N] = {7,2,3};
    int b[N] = {6,4,5};
    int c[N];

    int *d_a, *d_b, *d_c;

    cudaMalloc((void**)&d_a, sizeof(int)*N);
    // repeat for d_b and d_c

    cudaMemcpy(d_a, a, sizeof(int)*N, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b, sizeof(int)*N, cudaMemcpyHostToDevice);

    vector_add_cu<<<1,1>>>(d_c, d_a, d_b, N); // note: 1 thread

    cudaMemcpy(c, d_c, sizeof(int)*N, cudaMemcpyDeviceToHost);
    cudaFree(d_a);

    // repeat for d_b and d_c
}

__global__ void vector_add_cu(int *d_c, int *d_a, int *d_b, int n) {
    for (int i = 0; i<n; i++)
        d_c[i] = d_a[i] + d_b[i];
}
```

However, the code above does no parallel operations. We should either

- Launch 1 thread block with 3 threads
  ```
  vector_add_cu<<<1,3>>>(d_c, d_a, d_b);
  ```
- Or launch 3 thread blocks with 1 thread each
  ```
  vector_add_cu<<<3,1>>>(d_c, d_a, d_b);
  ```
- How do we know what is more suitable?
- How does each thread know the entries that it should compute?

## Specifying Thread using ThreadID

- With multiple threads running, we need to specify to each thread which elements (indices) of the vectors that it should compute
- If we launch 1 thread block with 3 threads, we use `threadIdx.x` to index the vector elements, each thread handles different indices

  ```c
  __global__
  void vector_add_cu(int *d_c, int *d_a, int *d_b, int n){
      d_c[threadIdx.x] = d_a[threadIdx.x] + d_b[threadIdx.x];
  }
  ```

- If we launch 3 thread blocks with 1 thread each, we use `blockIdx.x` to index the vector elements

  ```c
  __global__
  void vector_add_cu(int *d_c, int *d_a, int *d_b, int n){
      d_c[blockIdx.x] = d_a[blockIdx.x] + d_b[blockIdx.x];
  }
  ```

## Thread ID with Block ID

- In practice, we often use multiple blocks, each with multiple threads
  - Many problems lend themselves to 2D or 3D interpretation of the data
  - We hence identify a thread by combining its built-in variables `threadIdx` and `blockIdx` together with `blockDim`

```c
__global__
void vector_add_cu(int *d_c, int *d_a, int *d_b, int n){
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    d_c[i] = d_a[i] + d_b[i];
}
```

## Block vs Thread

- Parallel threads within a block have the additional mechanisms to:
  - Directly communicate and synchronise with each other
  - This feature is usually required in many real world applications

E.g. dot product computation

Given 2 vectors $a$ and $b$, we can find the angle of separation $\theta$ with the formula

$$
\theta = \arccos \frac{|a||b|}{a \cdot b} \\
a \cdot b = \sum_{i = 0}^{n - 1} a_i b_i
$$

Dot product can be effectively computed by launching parallel threads

- Each thread computes the product of corresponding pair of elements of the 2 vectors
- Then add (by using one of the threads) all the products to find the final sum

```c
Dot_prod_cu<<<3,1>>>(d_c, d_a, d_b);

__global__ void dot_prod_cu(int *d_c, int *d_a, int *d_b){
    int tmp[3];
    int i = blockIdx.x;
    tmp[i] = d_a[i] * d_b[i];
    if (i==0){
        int sum = 0;
        for (int j = 0; j < 3; j++)
            sum = sum + tmp[j];
        *d_c = sum;
    }
}

// the above does not actually work

Dot_prod_cu<<<1,3>>>(d_c, d_a, d_b);

__global__ void dot_prod_cu(int *d_c, int *d_a, int *d_b){
    int tmp[3];
    int i = threadIdx.x;
    tmp[i] = d_a[i] * d_b[i];
    if (i==0){
    int sum = 0;
    for (int j = 0; j < 3; j++)
        sum = sum + tmp[j];
    *d_c = sum;
    }
}

// the above does not actually work as well
```

These 2 examples do not work because there is no communication between threads/blocks

- There needs to be a way to access each other's data
- We can do that using shared memory (declared as `__shared__`)

```c
__global__ void dot_prod_cu(int *d_c, int *d_a, int *d_b){
    __shared__ int tmp[3];
    int i = threadIdx.x;
    tmp[i] = d_a[i] * d_b[i];

    if (i==0){
        int sum = 0;
        for (int j = 0; j < 3; j++)
            sum = sum + tmp[j];
        *d_c = sum;
    }
}
```

However, this may not fully work

- We need to synchronise all threads with one another
- Threads may not be ready to share its data
- We can use `__syncthreads()`

```c
__global__ void dot_prod_cu(int *d_c, int *d_a, int *d_b){
    __shared__ int tmp[3];
    int i = threadIdx.x;
    tmp[i] = d_a[i] * d_b[i];

    __syncthreads();

    if (i==0){
        int sum = 0;
        for (int j = 0; j < 3; j++)
            sum = sum + tmp[j];
        *d_c = sum;
    }
}
```

# Summary

- CUDA C is based off standard C, with extension to support NVIDIA GPU based parallel programming
- Consists of a host and a device
- Provides a general purpose parallel computing programming model based on using multiple threads running in parallel on the device
- Kernel entry point is indicated by using the `__global__` declaration
- Kernel is launched as parallel threads organised into thread blocks
- Threads within the same block can share memories to pass data, and synchronise with one another

# Software/Hardware Perspectives

- CUDA provides the programming model on how to create software with parallelism for the GPU that can scale transparently to large number of CUDA cores
- Threads, thread blocks are essentially a programmer's perspective
- However, how is a CUDA program executed when it is translated into its corresponding machine code?
  - This is handled by the CUDA runtime system, which knows the underlying architecture of the GPU

# Cores and Threads

- Recall that a GPU organises the streeaming processor (SP, or CUDA cores) into groups of SMs
- Each SM can contain from 32 SPs (Fermi) to 64 SPs (Turing)
- When a kernel is launched, each SP core will execute 1 thread
- How are threads assigned to cores?

## SM Block Allocation

- During runtime
  - Each block of threads can be scheduled on any of the available SM within a GPU, in any order, concurrently or sequentially
  - But each block cannot split among multiple SMs
- This is assigned by the runtime system
  - The runtime system knows about the internal detail of the underlying GPU
  - The runtime system enables automatic scalability
- There is also a maximum number of blocks that each SM can handle (e.g. 8)
  - But the actual number will be constrained by the available resources

## Block Vs Threads

- Earlier we asked which one was better: multiple parallel blocks with 1 thread each, or 1 block with multiple parallel threads?
- From the hardware resource perspective and performance consideration
  - Both options are not ideal
  - 1 block -> only 1 SM is used per GPU
  - 1 thread -> Potentially only 1 core is used per SM (if blocks spread among multiple SMs)
- GPU parallel computing is hence only effective if we have massive amounts of parallel data, which also compensates for overhead of copying data between host and device

# Warp

- In NVIDIA GPU, when an SM is given one or more thread blocks to execute, it first partitions the threads into groups of 32 known as a warp
  - All threads in a warp must execute the same instruction but with different data (SIMD)
  - Each warp then gets scheduled by a warp scheduler for execution (threads are always scheduled in a warp group)

## Warp Execution

- While a warp is waiting for the results of a previously executed instruction e.g. LD/ST data from memory, a different warp that is ready is selected for execution
  - This "hides" the latency
- So it is more efficient if we have many threads and many blocks
- Warp selection is based on certain prioritised scheduling policy, such as round robin, or least recently fetched

## SM with Multiple Warp Schedulers

- An SM can contain one or more warp schedulers
- E.g. An SM with dual warp schedulers, allowing 2 warps to be issued and executed concurrently

# SIMT Architecture

- SM employs a unique architecture called SIMT (hardware perspective)
  - Single instruction, multiple thread
  - I.e. a warp executes one common instruction for all its threads at a time
- Within a single thread, its instructions are
  - Pipelined to achieve instruction-level parallelism
  - Issued in order with no branch prediction and speculative execution
- Individual threads in a warp start together, at the same instruction address
  - But each thread has its own instruction address counter and registers
  - free to branch and execute independently when the thread diverges, such as due to data-dependent conditional execution and branches

# Hardware Resource Allocation

- On chip resources are used to store the execution context for each warp processed by an SM
  - E.g. program counters, registers, shared memory etc
- The context is maintained on-chip during the entire lifetime of the warp
  - Allows fast switching between warps
  - Without the need of the conventional CPU's context switching

## Resource Usage

- With limited amount of resources, such as the registers and shared memory in the SM
  - Number of threads that can actually be executed in an SM is capped for a given application
  - The more resources a thread requires, the less the number of threads that can simultaneously reside in the SM

E.g. Register Usage

- Suppose an SM can accommodate up to 1536 threads and has 16384 registers
- To accomodate 1536 threads, each thread can use no more than 16384 / 1536 = 10 registers
- If each thread requires 12 registers, then the number of threads that can simultaneously reside in the SM has to be reduced
- Reduction is done by removing a whole block
  - If each block contains 128 threads
  - Reduction of threads will be doen by reducing 128 threads at a time

E.g. Shared Memory Usage

- Suppose that a CUDA GPU has 24KB of shared memory per SM
- Suppose that each SM can support up to 8 blocks
- Each block must use no more than 24 / 8 = 3KB of shared memory
- If each block uses 5KB of shared memory in a particular application, then no more than 4 blocks can reside in an SM
- If there are not enough registers or shared memory available per SM for at least 1 block, then the kernel will fail to launch

Once a thread block launches on an SM

- All its warps are resident until their executions finish

Thus a new block is not launched on an SM

- Until there is sufficient number of free registers for all warps of the new block
- And until there is enough free shared memory for the new block

# Warp Issuing Efficiency

Each scheduler must have at least 1 warp eligible to issue an instruction every clock cycle (pipelined design). This helps hide latencies between instructions

Warp could be not ready due to

- Input operands not being ready
- Loading of data from global memory
- Waiting at synchronisation point

To avoid situations where all warps are stalled, and no instructions are issued

- Maintain as many active warps as possible through the execution of the kernel

# Summary

- Number of blocks and warps residing per SM for a given kernel call depends on
  - Execution configuration of the kernel call
  - Memory resources of the SM
  - Resources requirements of the kernel
- Due to the limited amount of resources in SM
  - The higher the demands of a thread, the lower the number of threads that can actually run in parallel inside the SM
  - Need to reduce the number of threads
- To extract the highest performance from the underlying architecture provided by the GPU, it is essential to
  - Have careful design of the program and its decomposition
  - Make judicious choice of the number of threads and blocks

# Resources

- https://medium.com/@smallfishbigsea/basic-concepts-in-gpu-computing-3388710e9239

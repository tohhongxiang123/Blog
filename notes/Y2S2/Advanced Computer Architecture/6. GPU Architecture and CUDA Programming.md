# GPU Architecture and CUDA Programming

- GPUs (Graphical Processing Unit) were designed to accelerate 3D graphics renderings on PC, such as for 3D games
- However, GPUs have now also been used extensively for mathematical and scientific computing purposes, such as AI applications
- One particular area that AI has been used extensively for is computer vision
    - Using machines to classify images, cluster similar images, perform object recognition etc.

# Convolutional Neural Networks (CNN) in AI

CNN is used extensively in many computer vision applications
- CNN based neural network techniques havfe been at the heart of spectacular advances in deep learning
- Particularly suitable to be implemented with GPUs
- CNNs involve massive amounts of convolutions/correlations
    - Hence, the effectiveness of a CNN depends on efficient implementation of matrix operations, mainly consisting of multiplication and addtion/accumulation (MAC) operations
    - Applying to independent sets of data, hence can be performed in parallel
    - Efficient if we have suitable processor architectures that support massive SIMD operations
        - SIMD is short for Single Instruction/Multiple Data, while the term SIMD operations refers to a computing method that enables processing of multiple data with a single instruction

# Recap: Data Level Parallelism

- Same operation is performed on multiple data values
    - Can be executed concurrently with multiple processing units
    - Single instruction multi data - SIMD
- SIMD execution
    - Vector processor
    - Array processor
    - SSE, AVX for multimedia support on modern CPUs
    - GPU

# Basic CPU Architecure: Single Core

- Designed for single-threaded code optimised for low latency
    - By using various performance enhancement techniques
    - Through sophisticated processor logic circuitry

# MultiCore CPU Architecture

Adding more (and simpler) cores
- Able to execute multiple instructions in parallel
- Effective if we can write efficient concurrent code (e.g. Pthread based multithreaded programming)

# CPU Architecture with SIMD ALUs

To better suport SIMD operations
- Duplicate ALU within each core
- All ALUs within each core must execute the same instruction simultaneously

# Basic GPU Architecture

- Consists of many cores, each with many ALUs (Streaming processors (SP)) to support SIMT (Single instruction, multiple threads)
- Enables massive parallel MAC (floating point) operations

# Evolution of GPU Computing

- Before GPU,
    - Graphics display is based on a simple framebuffer subsystem standard known as VGA (Video graphics array)
    - Graphics stored as bitmap based images in the frame buffer

# Fixed Function Pipelines

Original design of GPU
- Based on fixed function pipelines
- Configurable, but not programmable
- Inflexible

Host interface receives graphics API commands and data from CPU
- E.g. through DMA transfer

# Programmable Graphics Pipeline

Certain functions executed at a few graphics pipeline stages vary with rendering algorithms
- Motivated the hardware designers to make those pipeline stages programmable
- E.g. vertex shader and pixel shader

# Processors based GPU

- Introduces programmable vertex and pixel processors

# Unified GPU Architecture

Instead of separate processors for each processing type, use the same processor core

# GPU Processor Internals

- Basic GPU Core
- Add more ALUs
- Add more independent blocks of ALUs
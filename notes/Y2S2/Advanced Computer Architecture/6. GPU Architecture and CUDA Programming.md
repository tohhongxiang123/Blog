# GPU Architecture and CUDA Programming

- GPUs (Graphical Processing Unit) were designed to accelerate 3D graphics renderings on PC, such as for 3D games
- However, GPUs have now also been used extensively for mathematical and scientific computing purposes, such as AI applications
- One particular area that AI has been used extensively for is computer vision
    - Using machines to classify images, cluster similar images, perform object recognition etc.

# Convolutional Neural Networks (CNN) in AI

CNN is used extensively in many computer vision applications
- CNN based neural network techniques havfe been at the heart of spectacular advances in deep learning
- Particularly suitable to be implemented with GPUs
- CNNs involve massive amounts of convolutions/correlations
    - Hence, the effectiveness of a CNN depends on efficient implementation of matrix operations, mainly consisting of multiplication and addtion/accumulation (MAC) operations
    - Applying to independent sets of data, hence can be performed in parallel
    - Efficient if we have suitable processor architectures that support massive SIMD operations
        - SIMD is short for Single Instruction/Multiple Data, while the term SIMD operations refers to a computing method that enables processing of multiple data with a single instruction

# Recap: Data Level Parallelism

- Same operation is performed on multiple data values
    - Can be executed concurrently with multiple processing units
    - Single instruction multi data - SIMD
- SIMD execution
    - Vector processor
    - Array processor
    - SSE, AVX for multimedia support on modern CPUs
    - GPU

# Basic CPU Architecure: Single Core

- Designed for single-threaded code optimised for low latency
    - By using various performance enhancement techniques
    - Through sophisticated processor logic circuitry

# MultiCore CPU Architecture

Adding more (and simpler) cores
- Able to execute multiple instructions in parallel
- Effective if we can write efficient concurrent code (e.g. Pthread based multithreaded programming)

# CPU Architecture with SIMD ALUs

To better suport SIMD operations
- Duplicate ALU within each core
- All ALUs within each core must execute the same instruction simultaneously

# Basic GPU Architecture

- Consists of many cores, each with many ALUs (Streaming processors (SP)) to support SIMT (Single instruction, multiple threads)
- Enables massive parallel MAC (floating point) operations

# Evolution of GPU Computing

- Before GPU,
    - Graphics display is based on a simple framebuffer subsystem standard known as VGA (Video graphics array)
    - Graphics stored as bitmap based images in the frame buffer

# Fixed Function Pipelines

Original design of GPU
- Based on fixed function pipelines
- Configurable, but not programmable
- Inflexible

Host interface receives graphics API commands and data from CPU
- E.g. through DMA transfer

# Programmable Graphics Pipeline

Certain functions executed at a few graphics pipeline stages vary with rendering algorithms
- Motivated the hardware designers to make those pipeline stages programmable
- E.g. vertex shader and pixel shader

# Processors based GPU

- Introduces programmable vertex and pixel processors

# Unified GPU Architecture

Instead of separate processors for each processing type, use the same processor core

# GPU Processor Internals

- Basic GPU Core
- Add more ALUs
- Add more independent blocks of ALUs

# NVIDIA GPUs

Modern GPUs are especially well-suited to address non-graphics problems that can be expressed as *data parallel* computations with high arithmetic intensity

NVIDIA CUDA enabled GPU families
- CUDA programming platform
- Tesla (2008), Fermi, Kepler, Maxwell, Pascal, Volta, Turing (2018)

# Overview of NVIDIA GPU Architecture (Fermi)

![Nvidia Fermi Architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Fermi.svg/220px-Fermi.svg.png)

- Consists of 16 streaming multiprocessors
    - SMs are general purpose processors with a low clock rate target and a small cache. 
    - The primary task of an SM is that it must execute several thread blocks in parallel. 
    - As soon as one of its thread block has completed execution, it takes up the serially next thread block. 
    - In general, SMs support instruction-level parallelism but not branch prediction.
- Each SM contains 32 compute engines known as CUDA cores
    - CUDA (Compute Unified Device Architecture), is a specialized programming language that can leverage the GPU in specific ways to perform tasks with greater performance.
    - CUDA Cores are parallel processors, just like your CPU might be a dual- or quad-core device, nVidia GPUs host several hundred or thousand cores.
    - The cores are responsible for processing all the data that is fed into and out of the GPU, performing game graphics calculations that are resolved visually to the end-user. 
    - An example of something a CUDA core might do would include rendering scenery in-game, drawing character models, or resolving complex lighting and shading within an environment.
    - Each core has an integer ALU and FPU, which can execute a floating point/integer instruction per clock cycle
    - ![Fermi architecture with CUDA Core](https://miro.medium.com/max/1400/1*Wj6gB_MhhnmGu3OuToAjJg.jpeg)
- Each SM also has
    - 16 load/store units
    - 4 special function units (SFUs) to execute transecendental instructions (sin, cosine, reciprocal, square root)
    - 64KB of configurable shared memory and L1 cache

## Comparison with Nvidia Pascal GPU Architecture

- Total of 60 SMs with 3840 CUDA cores, arranged in the form of 
    - 6 GPC (Graphic processing clusters) that each contain 10 SMs, which consists of 64 CUDA cores per SM

## Comparison with Nvidia Turing GPU Architecture

- Total of 72 SMs with 4608 CUDA cores (and 576 tensor cores for matrix operations)

# GPU Computing with CUDA Programming

- CUDA is a general purpose parallel computing programming model, where a problem is first expressed in the form of multiple threads
    - Each thread is executed in parallel using the CUDA cores in the GPU
    - Enables many complex computational problems to be solved in a very efficient way

In a CUDA program
- The problem is first decomposed into sub-problems
- Each of which is allocated to a "thread block"

Each sub problem is then separated into finer pieces that can be solved cooperatively in parllel using multiple threads within the thread block

# CUDA Programming Model

- CUDA operates on a heterogeneous programming model, consisting of a host and a device
    - Host is typically a CPU and its memory (host memory)
    - Device is the GPU and its memory (device memory)
- A program will start and is run by the host
    - The host launches the parallel threads which are executed on the device

# Basic CUDA C Program - Hello world

```c
int main(void) {
    printf("Hello, World!");
    return 0;
}
```

This is a simple CUDA C program, which only runs on the host, just consisting of standard C statements
- Program file is stored with the `.cu` extension (e.g. `hello_world.cu`)
- This `.cu` program file is compiled using the NVIDIA compiler `nvcc` (`nvcc hello_world.cu -o hello_world`)

## Device Code (Kernel)

To indicate the code that is to run on the device, we use the CUDA C keyword `__global__` declaration specifier

```c
__global__ void hello_GPU(void) {
    printf("Hello from GPU");
}
```

Device code are launced as kernel in CUDA, which can be called from the host code as follows

```c
int main(void) {
    hello_GPU<<<1, 1>>>();
    printf("Hello from CPU");
    return 0;
}
```

## Behind the Scenes

- During compilation, the source file is split into host and device components
- Device code is compiled by NVIDIA's compiler (`__global__` codes segments)
- Host code is compiled by using standard host compiler, such as GCC

## Kernel and Parallel Threads

- A kernel is an extended C function
    - It can be executed N times in parallel by N different CUDA threads when called (as opposed to only once for regular C functions)
- The number of parallel CUDA threads launched for the kernel is specified in thehost code using triple angle brackets syntax
    - `hello_dev<<<2, 4>>>();`
    - This is known as the execution configuration syntax
- The 2 parameters between the brackets specify how the threads are to be launched
    - Number of thread blocks (2 for above)
        - A thread block is a programming abstraction that represents a group of threads that can be executed serially or in parallel
        - For better processing and data mapping, threads are grouped into thread blocks
    - Number of threads in each block (4 for above)
    - `kernel<<<2, 4>>>();` launches 2 thread blocks, each with 4 threads
        - Total of 8 threads, executing 8 copies of the kernel
        - On the latest NVIDIA GPUs, a thread block may contain up to 1024 threads

## Thread ID

- How to identify different threads?
    - Each thread that executes a copy of the kernel is given a unique thread ID within the block
    - Thread ID is accessible within the kernel through the built-in `threadIdx` variable

    ```c
    __global__ void hello_GPU(void) {
        int i = threadIdx.x;
        printf("Hello from GPU[i]!\n");
    }
    ```

    - Output:

    ```
    Hello from GPU[0]!
    Hello from GPU[1]!
    Hello from GPU[2]!
    Hello from GPU[3]!
    ```

## Block ID

- With multiple thread blocks, each block executes the kernel is also given a unique block ID
    - accessible within the kernel through the built-in `blockIdx` variable
- Threads among different blocks can then be identified through the `threadIdx` and `blockIdx`, together with the built-in variable `blockDim` - which corresponds to the number of threads per block
    - Eg: Launching `hello_GPU<<<2, 4>>>()`

    ```c
    __global__ void hello_GPU(void){
        int i = blockIdx.x * blockDim.x + threadIdx.x;
        printf(“Hello from GPU[i]!\n”);
    }
    ```

## Multidimension Threads and Blocks

- Why are threads and blockIds extended with `.x`?
- CUDA threads and blocks can be defined to be of 1-dimensional, 2-dimensional (x and y) and 3-dimensional (x, y, z)
    - Multidimensional blocks are suitable for addressing complex proboelm best described in multi-dimensions (e.g. Matrix)

# Synchronisation between Host and Device

```c
__global__ void hello_GPU(void) {
    printf("Hello world from GPU\n");
}

int main(void) {
    hello_GPU<<<1, 4>>>();
    printf("Hello from CPU");
    return 0;
}
```

- The program above will most likely not work properly
    - The `printf` might not run in the order we want to, because the CUDA cores are asynchronous
- When we need the host to wait for all the threads to complete the kernels execution, use the CUDA function `cudaDeviceSynchronize()`

# Passing of Paremeters

We will look at performing vector addition 

- In practice, we often need to pass parameters from the host to the device for computation
- We have to also collect the results back from the device to the host

Consider the addition of 2 vectors (a + b)
- Result is another vector c

## Vector addtion (Host only)

```c
int main(void){
    int N = 3;
    int a[N] = {7,2,3};
    int b[N] = {6,4,5};
    int c[N];
    vector_add(&c[0], &a[0], &b[0], N);
    return 0;
}

void vector_add(int *h_c, int *h_a, int *h_b, int n){
    for (int i = 0; i < n; i++)
        h_c[i] = h_a[i] + h_b[i];
} 
```

## Memory Management between Host and Device

This vector addition is obviously most suitable to be executed by using the GPU
- We can launch `n` threads, for each of the entry in the vector
- But we have to first pass data from the host to the device (GPU)
    - This is done using the CUDA memory management functions: `cudaMalloc()`, `cudaMemcpy()`, and `cudaFree()`
- Similar to the standard C equivalent functions `malloc()`, `memcpy()`, and `free()`

```c
int main(void){
    int N = 3
    int a[N] = {7,2,3};
    int b[N] = {6,4,5};
    int c[N];

    int *d_a, *d_b, *d_c;

    cudaMalloc((void**)&d_a, sizeof(int)*N);
    // repeat for d_b and d_c

    cudaMemcpy(d_a, a, sizeof(int)*N, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b, sizeof(int)*N, cudaMemcpyHostToDevice);

    vector_add_cu<<<1,1>>>(d_c, d_a, d_b, N); // note: 1 thread

    cudaMemcpy(c, d_c, sizeof(int)*N, cudaMemcpyDeviceToHost);
    cudaFree(d_a);

    // repeat for d_b and d_c
} 

__global__ void vector_add_cu(int *d_c, int *d_a, int *d_b, int n) {
    for (int i = 0; i<n; i++)
        d_c[i] = d_a[i] + d_b[i];
}
```

However, the code above does no parallel operations. We should either
- Launch 1 thread block with 3 threads
    ```
    vector_add_cu<<<1,3>>>(d_c, d_a, d_b);
    ```
- Or launch 3 thread blocks with 1 thread each
    ```
    vector_add_cu<<<3,1>>>(d_c, d_a, d_b);
    ```
- How do we know what is more suitable?
- How does each thread know the entries that it should compute?

## Specifying Thread using ThreadID

- With multiple threads running, we need to specify to each thread which elements (indices) of the vectors that it should compute
- If we launch 1 thread block with 3 threads, we use `threadIdx.x` to index the vector elements, each thread handles different indices

    ```c
    __global__
    void vector_add_cu(int *d_c, int *d_a, int *d_b, int n){
        d_c[threadIdx.x] = d_a[threadIdx.x] + d_b[threadIdx.x];
    }
    ```
- If we launch 3 thread blocks with 1 thread each, we use `blockIdx.x` to index the vector elements

    ```c
    __global__
    void vector_add_cu(int *d_c, int *d_a, int *d_b, int n){
        d_c[blockIdx.x] = d_a[blockIdx.x] + d_b[blockIdx.x];
    }
    ```

## Thread ID with Block ID

- In practice, we often use multiple blocks, each with multiple threads
    - Many problems lend themselves to 2D or 3D interpretation of the data
    - We hence identify a thread by combining its built-in variables `threadIdx` and `blockIdx` together with `blockDim`

```c
__global__
void vector_add_cu(int *d_c, int *d_a, int *d_b, int n){
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    d_c[i] = d_a[i] + d_b[i];
}
```

## Block vs Thread

- Parallel threads within a block have the additional mechanisms to:
    - Directly communicate and synchronise with each other
    - This feature is usually required in many real world applications

E.g. dot product computation

Given 2 vectors $a$ and $b$, we can find the angle of separation $\theta$ with the formula

$$
\theta = \arccos \frac{|a||b|}{a \cdot b} \\
a \cdot b = \sum_{i = 0}^{n - 1} a_i b_i
$$

Dot product can be effectively computed by launching parallel threads
- Each thread computes the product of corresponding pair of elements of the 2 vectors
- Then add (by using one of the threads) all the products to find the final sum


```c
Dot_prod_cu<<<3,1>>>(d_c, d_a, d_b);

__global__ void dot_prod_cu(int *d_c, int *d_a, int *d_b){
    int tmp[3];
    int i = blockIdx.x;
    tmp[i] = d_a[i] * d_b[i];
    if (i==0){
        int sum = 0;
        for (int j = 0; j < 3; j++)
            sum = sum + tmp[j];
        *d_c = sum;
    }
}

// the above does not actually work

Dot_prod_cu<<<1,3>>>(d_c, d_a, d_b); 

__global__ void dot_prod_cu(int *d_c, int *d_a, int *d_b){
    int tmp[3];
    int i = threadIdx.x;
    tmp[i] = d_a[i] * d_b[i];
    if (i==0){
    int sum = 0;
    for (int j = 0; j < 3; j++)
        sum = sum + tmp[j];
    *d_c = sum;
    }
}

// the above does not actually work as well
```

These 2 examples do not work because there is no communication between threads/blocks
- There needs to be a way to access each other's data
- We can do that using shared memory (declared as `__shared__`)

```c
__global__ void dot_prod_cu(int *d_c, int *d_a, int *d_b){
    __shared__ int tmp[3];
    int i = threadIdx.x;
    tmp[i] = d_a[i] * d_b[i];

    if (i==0){
        int sum = 0;
        for (int j = 0; j < 3; j++)
            sum = sum + tmp[j];
        *d_c = sum;
    }
}
```

However, this may not fully work
- We need to synchronise all threads with one another
- Threads may not be ready to share its data
- We can use `__syncthreads()`

```c
__global__ void dot_prod_cu(int *d_c, int *d_a, int *d_b){
    __shared__ int tmp[3];
    int i = threadIdx.x;
    tmp[i] = d_a[i] * d_b[i];

    __syncthreads();

    if (i==0){
        int sum = 0;
        for (int j = 0; j < 3; j++)
            sum = sum + tmp[j];
        *d_c = sum;
    }
}
```

# Summary

- CUDA C is based off standard C, with extension to support NVIDIA GPU based parallel programming
- Consists of a host and a device
- Provides a general purpose parallel computing programming model based on using multiple threads running in parallel on the device
- Kernel entry point is indicated by using the `__global__` declaration
- Kernel is launched as parallel threads organised into thread blocks
- Threads within the same block can share memories to pass data, and synchronise with one another

# Resources

- https://medium.com/@smallfishbigsea/basic-concepts-in-gpu-computing-3388710e9239
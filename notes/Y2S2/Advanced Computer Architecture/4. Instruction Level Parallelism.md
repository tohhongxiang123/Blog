# Instruction Level Parallelism

# Single-cycle vs Multi-cycle

-   Single cycle: Each instruction takes a single cycle
-   Multi cycle: Each instruction can take more than one cycle
-   Advantage of multi-cycle instructions: Shorter clock period
-   However, multi-cycle instructions result in high Cycles-per-instruction (CPI)

# Pipeline Datapath

-   Pipelining allows multiple sub-tasks to be carried out simultaneously using independent resources
-   Need to balance time taken by each sub-task

# Ideal Pipelining

![Pipelining](https://microchipdeveloper.com/local--files/32bit:mx-arch-pipeline/pic32mx-pipelined-execution.png)

-   In an n-stage pipeline, the CPU can operate a maximum of $n$ times faster than without a pipeline
-   However, this is not realistic, as there is an increase in latency due to register delays

# Challenges in Instruction Pipelining Realisation

Ideal Pipeline

-   Identical task of all instructions
-   Uniform decomposition of tasks
-   Independent computations

Real Pipeline

-   Except load instructions, all other instructions do not need all stages
-   Not all pipeline stages involve the same time to complete their respective sub tasks
-   Execution of one instruction may depend on one of the preceding instructions

Complications

-   Datapath: Many instructions in flight
-   Control: Must correspond to multiple instructions
-   Instructions may have data and control flow dependencies
-   Instructions may have to stall and wait for another instruction to complete execution first

# Fundamental Issues in Pipelining

-   Balancing work in pipeline stages
    -   How many stages
    -   What is done in each stage
-   Keeping pipeline correct, moving and full in the presence of events that disrupt pipeline flows
    -   Handling dependencies
        -   Data
        -   Control
    -   Handling resource contention
-   Advanced: Improving pipeline throughput
    -   Minimising stalls

# Pipeline Hazards

-   Stalling of the pipeline: Drop in efficiency
-   Various types of hazards
    -   Data hazard
    -   Control/instruction hazard
    -   Structural hazard
-   Structural hazard: If 2 different instructions require the use of a given hardware resource at the same time that will lead to a stall in the pipeline
    -   E.g. If we only have 1 memory, and one instruction is writing to the memory, however, the other instruction is trying to read from the same location in memory
-   Data hazard: Either the source or destination register of an instruction are not available at the time expected in the pipeline. Hence a pipeline stall occurs
-   Control/Instruction hazard: Conditional and unconditional jumps, subroutine calls, and other program control instructions can stall a pipeline, due to a delay in the availability of an instruction
    -   If missed, the instruction has to be fetched from main memory, which introduces a delay in the instruction fetch, and the pipeline stalls
    -   ![Pipeline control hazard](https://ars.els-cdn.com/content/image/3-s2.0-B9780123944245000070-f07-54-9780123944245.jpg)
    -   Instructions that were already loaded are not executed, and a new instruction has to be loaded, causing a stall in the pipeline

# Solutions to Solve Pipeline Hazards

-   Structural Hazard
    -   Can be solved by additional hardware elements
-   Data Hazards
    -   Need to know dependencies between data in the program and eliminate them
-   Control Hazard
    -   Need to predict the location of conditional/unconditional branch to avoid stalling

# Data Dependencies

A data dependency is a situation in which an instruction refers to the data of a preceding statement

Type of data dependencies

-   True/flow dependence (RAW - read after write) - $j$ cannot execute until $i$ produces its results ($i$ and $j$ are 2 different instructions)

    ```
    SUB X3, X2, X1
    AND X5, X3, X4
    ```

    -   `AND` has to wait for `SUB` to finish writing its result, before the `AND` statement can read the value of `X3`

-   Output dependence (WAW - write after write) - $j$ cannot write its result until $i$ has written its result

    ```
    ADD X0, X2, X1
    SUBI X0, X3, X1
    ```

    -   `SUBI` has to wait for `ADD` to write to `X0` before `SUBI` can write its own result to `X0`

-   Anti dependence (WAR - write after read) - $j$ cannot write its results until $i$ has read its sources

    ```
    ADD X0, X2, X1
    ADDI X1, X3, #2
    ```

    -   `ADDI` has to wait for `ADD` to read the data from `X1` before it can write to `X1`

## How to Handle Data Dependencies

-   Anti and output dependences are easier to handle
-   True (Flow or RAW) dependeces are more difficult, as they constitute true dependence on a value
-   5 ways to handle true dependences

    1. Detect and wait until value is available in register
        - Stall the program (Hardware)
        - Compiler can also plug in the NOP instruction in between instructions (Software)
    2. Detect and forward/bypass data to dependent instruction
    3. Detect and eliminate dependence at the software level
        - No need for hardware to detect dependence
    4. Predict the neede values, execute "speculatively", and verify that execution is correct
    5. Do something else first (fine-grained multithreading)
        - No need to detect data dependencies

### Detect and Wait

CPU can either stall:

![Detect and wait](https://www.massey.ac.nz/~mjjohnso/notes/59304/Image166.gif)

Or introduce NOPs into the pipeline

![NOP](https://www.massey.ac.nz/~mjjohnso/notes/59304/Image167.gif)

### Data Forwarding - Through Register

![Data forwarding through register](https://www.massey.ac.nz/~mjjohnso/notes/59304/Image168.gif)

-   Write and read in the same cycle
-   Note that
    -   The register read happens combinationally (No clock required)
    -   The register will be valid during the time it is read
    -   The value returned will be the value written in the earlier clock cycle
    -   The write of the register file occurs on the clock edge
    -   If we want to read and return the value currently being written, additional logic required

![Data forwarding](https://www.massey.ac.nz/~mjjohnso/notes/59304/Image169.gif)

### Detect and Forward/bypass

-   Forward data from pipeline stage register to function units which require the data
-   Forward data only in the clock cycle when they are required

### In-Order and Out-of-Order Execution

-   In-order instruction execution
    -   Instructions are fetched, executed and completed in compiler generated order
    -   If one instruction stalls, all the instructions stall
    -   Instructions are statically scheduled
-   Out-of-order instruction execution
    -   Instructions are fetched in compiler-generated order
    -   But they may be executed in another order
    -   Independent instructions behind a stalled instructions may pass it
    -   Instructions are dynamically scheduled during runtime

### Dynamic Scheduling

Out of order processors

-   After instruction decode
    -   Check for structural hazards
        -   An instruction can be issued when a funtional unit is available
        -   An instruction stalls if no appropriate functional unit is free in that clock cycle
    -   Check for data hazards
        -   An instruction can execute if its operand has been loaded in the register
        -   An instruction stalls if the operands are not available
    -   Don't wait for previous instructions to execute if this instruction does not depend on them, i.e. independent ready instructions can execute before earlier instructions that are stalled

For example:

In-order processors:

```
LDUR X1, [X4, #100]
ADD X2, X1, X4 // (need to wait until the value is loaded)
SUB X5, X6, X7 // independent of the other 2 instructions
```

Out-of-order processors:

```
LDUR X1, [X4, #100]
SUB X5, X6, X7 // independent of the other 2 instructions, hence can run first
ADD X2, X1, X4 // X1 has loaded, hence can now execute, no stalls
```

### Instruction Reordering

```
LDUR X1, [X0, #0]
LDUR X2, [X0, #4]
ADD X3, X1, X2 // stalled due to previous instruction
STUR X3, [X0, #12]
LDUR X4, [X0, #8]
ADD X5, X1, X4 // stalled due to previous instruction
STUR X5, [X0, #16]
```

By reordering the instructions, we can prevent stalls

```
LDUR X1, [X0, #0]
LDUR X2, [X0, #4]
LDUR X4, [X0, #8]
ADD X3, X1, X2 // no more stall since x1 and x2 already loaded
STUR X3, [X0, #12]
ADD X5, X1, X4 // no more stall since x1 and x4 already loaded
STUR X5, [X0, #16]
```

### Register Renaming

```
ADD X4, X2, X1;
ANDI X1, X0, #2; // stalled due to write-before-read
```

If we used `X3` instead of `X1`:

```
ADD X4, X2, X1;
ANDI X3, X0, #2; // no more stall
```

However, by using another register instead, more register resources are required

### Loop Unrolling

-   Loop unrolling leads to multiple replications of the loop body
    -   Unrolling creates longer code sequences
    -   However, the goal is to execution iterations in parallel

For example:

```c
for (int i = 0; i < 16; i++) {
    c[i] = a[i] + b[i];
}
```

When unrolled:

```c
for (int i = 0; i < 7; i++) {
    c[2*i] = a[2*i] + b[2*i];
    c[2*i + 1] = a[2*i + 1] + b[2*i + 1];
}
```

-   This causes greater demand for registers
    -   Higher register pressure: More concurrency demands for more resources
    -   How can loop unrolling help us reduce stalls and improve CPI?

```
Loop:   LDUR X0, [X1, #0];      // load to X0 from mem[0+X1]
        ADD X4, X0, X2;         // add [X0]+[X2], stalled due to waiting for LDUR
        STUR X4, [X1, #0];      // store X4 to mem[0+X1], no stall if data forwarding is allowed
        SUBI X1, X1, #8;        // decrement pointer 8, no stall if data forwarding is allowed
        CBNZ X1,Loop;           // branch X1!=zero, no stall if data forwarding is allowed
```

Average CPI = (Number of instructions + Number of stalls) / (Number of instructions) = (5 + 1) / 5 = 1.2

By unrolling,

```
Loop:   LDUR X0, [X1, #0]
        LDUR X6, [X1, #-8]
        LDUR X10, [X1,#-16]
        LDUR X14, [X1,#-24]
        ADD X4, X0, X2
        ADD X8, X6, X2
        ADD X12, X10, X2
        ADD X16, X14, X2
        STUR X4, [X1, 0]
        STUR X8, [X1, #-8]
        STUR X12, [X1, #-16]
        STUR X16, [X1, #-24]
        SUBI X1, X1, #32
        CBNZ X1,LOOP
```

No more stalling occurring. Avg CPI = 1

# Control Hazards

-   Branching instructions
    -   Pipeline has to stall because it changes the exeuction sequence of the instrutions
-   Unconditional and conditional branches: In decode stage of branch, the decoding unit comes to know that there is an alteration to the program's control flow
    -   Incorrectly fetched instruction by the instrution fetch unit must be discarded
    -   New instruction specified by the conditional and unconditional branch must be fetched by the instruction unit. This causes stalls (bubble) in the pipeline, e.g. `B` and `CBZ`
-   The time lost as a result of branch instruction is often referred to as the **branch penalty**

## How to Tackle Control Hazards

-   Conservative way
    -   Stall pipeline immediately after we fetch a branch instruction, waiting until the pipeline determines the outcome of the branch and knows the address of the next instruction to be fetched
    -   3 stalls when the branch target address is updated in memory stage (Instruction decode, instruction fetch, execute is wasted)
    -   If branch target address is udpated in execute stage, then only 2 stalls instead
-   Branch prediction
    -   Simple way is to predict always that the branches will be untaken
    -   Only when branches are taken does the pipeline stall

## Branch Prediction

-   Branch prediction deals with branch penalty by continuing executing the instructions following the branch speculatively
    -   If branch does not occur, then no pipeline stall
    -   If branch occurs, pipeline must be flushed (pre-loaded instructions are removed from the pipeline)
    -   On average, reduces pipeline stalls by 50%
-   **Static branch prediction** techniques: The actions for the branch are fixed for each branch during the entire execution (where behavior is highly predictable)
-   **Dynamic branch prediction** techniques: The prediction behavior decision may change depending on the execution history (where behavior is not predictable)

### Need for Branch Prediction

-   Modified LEGv8 pipeline with branch address and outcome calculated in second stage (needs only 1 stall)
    -   With the branch decision made during Instruction Decode stage, there is a reduction of the cost associated with each branch (branch penalty)
    -   We need only 1 clock cycle stall after each branch
    -   Or a flush of only 1 instruction following the branch
    -   One-cycle-delay for every branch still yields a performance loss of 10% to 30% depending on branch frequency
    -   Pipeline stall cycles per instruction due to branches = branch frequency \* branch penalty

### Static Prediction

Static branch prediction is used in processors where the expectation is that the branch behaviour is highly predictable at compile time

-   Branch always not taken - predict-not-taken (Speculation)
    -   Execute successive instructions in sequence
    -   Flush the pipeline and read correct instructions if the branch is actually taken
-   Branch always taken - predict-taken
    -   The predicted taken scheme makes sense for pipelines where the branch target address is known before the branch outcome
    -   However in LEGv8 pipeline we haven't calculated yet the branch address, still incurs branch penalty
-   Delayed branch

    -   Compiler statically schedules an independent instruction in the **branch delay slot**
    -   The instruction in the branch delay slot is executed whether or not the branch is taken
    -   If we assume a branch delay of 1 cycle (as for modified LEGv8), we have only 1 delay slot
    -   LEGv8 compiler always schedules a branch independent instruction after the branch

    ```
    CBZ X1 L1
    ADD X4,X5,X6 // schedule an independent instruction directly after the branch
    LDUR X3, [X0, #300]
    LDUR X7, [X0, #400]
    LDUR X8, [X0, #500]
    ```

    -   The behavior of the delayed branch is the same, whether or not the branch is taken
        -   If branch is untaken, then execution continues with the instruction after the branch
        -   If branch is taken, then execution continues at the branch target

### Dynamic Prediction

Dynamic prediction usually relies on some measure of the past behavior to predict the future

-   When CPU sees a branch, it uses a hardware predictor to make a decision of which path to speculate on
-   Later on when the branch outcome is known, it updates the predictor

#### One-bit Dynamic Prediction

Single T bit (Last time predictor)

-   T is a single bit, used to predict whether the next branch or not
-   If T is currently 1, predict that the next branch is taken
-   If T is currently 0, predict the next branch is not taken
-   Update T again based on the result of the branch afterwards
-   Consider that you start from predict "not taken"
    -   TTTTTTTTTTNNNNNNNNNN: 90% accuracy
    -   Always mis-predicts the last iteration and the first iteration of a loop branch
    -   TNTNTNTNTNTNTNTNTNTN: 0% accuracy

#### Improving One-bit Detector

Problem: A last-time predictor changes its prediction from taken (T) to not taken (NT) or vice versa too quickly, even though the branch may be mostly taken or mostly not taken

![2 bit branch predictor](https://i.stack.imgur.com/BfCk8.png)

Solution: Add hysteresis to the predictor so that prediction does not change on a single different outcome

-   Use 2 bits to track the history of predictions for a branch instead of a single bit
-   Can have 2 states for T or N instead of 1 state each
-   TNTNTNTNTNTNTNTN: 50% accuracy
-   Assuming initial probable (weakly taken)
-   Disadvantage: More hardware

Another solution is **bimodal prediction**, which uses a counter and the state of that counter determines the prediction

-   Generalised scheme of 3 bit predictor
-   The counter is incremented if branch is taken
-   The counter is decremented if branch is not taken
-   Counters saturate (no wraparound). The speculation decision is based on the most significant bit: If MSB = 1, then counter is above halfway
    -   MSB = 1, then predict branch is taken
    -   MSB = 0, then predict branch is not taken

000 -> 001 -> 010 -> 011 -> 100 -> 101 -> 110 -> 111

# Instruction Level Parallelism (ILP)

-   Execution of more than one instruction at the same time (in parallel)
-   Multi-issue datapath: Datapath that allows execution of 2 or more instructions in parallel
-   N-issue architecture: Executes N instructions at the same time

## ILP and Multi-issue procesors

-   1 instruction issued in a clock cycle -> single-issue processor
-   2 or more instructions are issued in a clock cycle -> multi-issue/super scalar processor

## Instruction Level Parallelism

Parallel exeuction requires 3 major tasks

1. Which instructions should be executed in parallel?
    - Checking the data dependence between instructions to identify the instructions which can be grouped together for parallel execution
2. Where to be executed?
    - Assigning instructions to different functional units in the processor
3. When to be executed?
    - Determining when instruction execution is to be initiated

Two approaches to achieve ILP

1. Hardware approach: Superscalar processor
    - P5 Pentium, the first superscalar x86 processor
    - Most general purpose CPUs developed since 1998 are superscalar
2. Software (or compiler-based) approach: Very Long Instruction Word (VLIW) processor
    - VLIW CPUs contain multiple RISC like functional units (FU). Typically have 4 to 8 FUs

### Concept of Superscalar Processing

1. Fetch multiple instructions in parallel
2. Decode multiple instructions in parallel
3. Dispatch each instruction to its respective functional unit
4. Each functional unit executes its respective instruction in parallel with the other functional units

Requirements for superscalar processing

-   Multiple functional units in a CPU are used to execute more than one instruction concurrently in a cycle
-   More than 1 independent instructions need to be available to be executed (Makes use of ILP)
-   Use specialised hardware to issue multiple independent instructions that can be executed simultaneously

### Out-of-Order Execution in Superscalar Pipeline

-   Instructions are fetched in compiler-generated order (in-order)
-   Instruction completion may also be in order
-   Instructions are executed in some other order: Independent instructions behind a stalled instruction can be executed prior to the stalled instruction
-   Dynamically scheduled: The order of execution of instructions is changed

## Methods to Extract More Parallelism

1. Instruction reordering and out-of-order execution
    - Change the order of execution of instruction if it does not violate data dependence
2. Speculative execution with dynamic scheduling
    - To execute an instruction without exactly knowing if that instruction needs to be executed: Ahead of branch outcome
3. Loop unrolling
    - Execute instructions as soon as dependencies are satisfied and functional units are available

### Loop Unrolling for ILP

-   Loop unrolling leads to multiple replications of the loop body
    -   Multiple independent instructions that can be executed in parallel

```
Loop:   LDUR X0, [X1, #0] // 2 stalls
        ADDI X0, X0, #25 // 2 stalls
        STUR X0, [X1, #0]
        ADDI X1, X1, #8
        SUBI X2, X2, #1 // 2 stalls
        CBNZ X2, Loop  // 1 stall
```

-   Control hazard is removed in decode stage (1 stall cycle)
    -   CPI = (6 + 7) / 6 = 2.17 without reordering in a scalar processor

|      | Way-1           | Way-2             | Cycle |
| ---- | --------------- | ----------------- | ----- |
| Loop | SUBI X2, X2, #1 | LDUR X0, [X1, #0] | 1     |
|      | nop             | nop               | 2     |
|      | nop             | nop               | 3     |
|      | ADDI X0, X0, #8 | nop               | 4     |
|      | ADDI X1, X1, #8 | nop               | 5     |
|      | CBNZ X2, Loop   | nop               | 6     |
|      | nop             | STUR X0, [X1, #0] | 7     |

## Different Stages in Superscalar Pipeline

![Stages of superscalar pipeline](https://www.researchgate.net/profile/Marcelo-Brandalero/publication/273447010/figure/fig4/AS:391801589518337@1470424186106/Program-execution-on-a-superscalar-processor.png)

-   Fetch
    -   Fetch multiple instructions from instructino memory in every clock cycle and store in instruction buffer
-   Decode
    -   Decode instructions available in the instruction buffer, and sent to dispatch buffer
-   Dispatch
    -   Instructions are issued from dispatch buffer into functional units as and when their operands and respective functional units are available
-   Execute
    -   Instructions are marked "finished" when executed, and enter the reorder buffer (in out-of-order)
-   Complete
    -   Instructions exit the reorder buffer in order. Results are written in the processor registers in order
-   Retire
    -   Performs memory update if any (usually in D-cache)

![Superscalar process](https://ars.els-cdn.com/content/image/3-s2.0-B9780123944245000070-f07-68-9780123944245.jpg)

## VLIW Processing Strategy

-   Packs multiple independent operations into 1 instruction
-   VLIW processor needs a compiler to break the program instructions down into basic operations that can be performed by the processor in parallel
-   These operations are put into a very long instruction word which the processor (consisting of multiple functional units) get executed in the appropriate functional units
-   There must be enough parallelism in the program code to fill the available slots to reduce the CPI significantly

### VLIW Architecture

![VLIW architecture](https://media.geeksforgeeks.org/wp-content/uploads/20201030224132/BlockDiagramofVLIW.jpg)

-   Multiple operation execution
-   Many functional units: Each is executing its own instruction
-   Typically maximum of 4 or 8 instructions per cycle are issued

### Advantages and Disadvantages of VLIW

-   Compiler prepares fixed packets of multiple operations: Gives the complete plan of execution
    -   Dependencies are determined by compiler, and that is used to schedule according to the latencies of corresponding functional units
    -   Functional units are assigned by the compiler, they correspond to the position within the instruction packet or instruction slot
    -   Compiler produces fully-scheduled, hazard-free code -> Hardware need not check dependencies for scheduling
-   Compatability issues across implementations is a major problem
    -   VLIW code cannot run properly with different number of functional units or functional units with different latencies
    -   Unscheduled events (e.g. cache miss) can stall the entire processor
-   Code density is an important concern
    -   Slot utilisation varies: NOPs increase with branching (control)
    -   Reduce NOPs by compression (Flexible/variable-length VLIW)

# Memory Systems

# Cache Design

- Memory
    - An ocean of bits
    - Many technologies are available, such as SRAM, DRAM, Magnetic disks
- Key issues of memory
    - Placement (where the bits are stored, where can a block of memory be placed?)
    - Identification (finding the right bits, how do I find a block of memory?)
    - Replacement (finding space for new bits, which block should I remove if there is a miss?)
    - Write policy (propagating changes to bits, how do I propagate changes or keep memory updated?)

# How Placement is Done in Cache?

![Cache memory in CPU](https://www.rfwireless-world.com/images/L1-Cache-vs-L2-Cache-vs-L3-Cache.jpg)

- L1 cache (level 1 cache)
    - Generally very small
    - Typically 8K or 16K
    - Resides in the processor
- L2 cache
    - Typically 256K or 512K
    - Resides between CPU and main memory
    - Slower than L1 cache

Modern CPUs normally have built-in L1, L2, and even L3 cache

What makes cache special?
- It is not accessed by address
- It is accessed by content
- Content addressable memory (CAM)

Hence cache mapping schemes are required
- Cache entries are checked to see if the value requested is stored in the cache
- Needs to convert the main memory address into the cache location

# How to Address the Cache

- Main memory and cache are divided into blocks of the same size
- Each block maps to a location in the cache, determined by the index bits in the main memory address (tag, index, offset)
- In the cache, the cache lines are divided into different fields (valid, tag, cache block)
- The valid bit pertaining to each cache line tells status of the data in the cache - indicates if the data is valid


# General Organisation of a Cache

![General organisation of a cache](https://slideplayer.com/slide/14503887/90/images/11/General+Organization+of+a+Cache.jpg)

- A cache is an array of sets
- Each set contains 1 or more lines
- Each line holds a block of data

To find the content at a specific location, we receive an address `A`. `A` is made up of `m` bits,
- `i` bits for the index(Size of `i` = $log_2 n$, where $n$ is the number of sets in the cache)
- `o` bits for the offset (Size of `o` = $log_2 B$, where $B$ is the size of the block in the cache)
- `t` bits for the tag (Size of `t` = address size - `i` - `o`)

Each cache line is made up of
- `v` - 1 bit telling whether data on that particular cache line is valid
- `tag` - the tag number of that cache line
- `block` - the actual data on the block

1. We locate the set based on `index`. The cache set with `set == index` is the correct line to look for
2. We check whether the line is valid by looking at the `v` bit on the cache line
3. Within the cache set, we locate the line on the set based off the `tag` bits. If `address.tag == set.tag`, we have found the correct line
4. Locate the data in the line based on the `offset` in the address

# Cache Design: Placement Policy

- Mapping of the main memory blocks to the cache lines
    - To decide where in the cache a copy of a selected memory block will reside
    - Mapping schemes determines where the data is palced when it is originally copied into cache
    - It also provides a method for the CPU to find previously copied data when searching cache

Placement policies include:

- Direct-mapped cache
    - Simplest and fastest address mapping
    - A memory block is mapepd to a specific cache line only
- Fully associative cache
    - A memory block can be placed in any of the cache lines
    - Flexible but expensive
- Set-associative cache
    - Combines strategy of fully associative and direct mapped caches
    - Popularly used

## Direct-Mapped Cache

![Direct mapped cache](http://www.mathcs.emory.edu/~cheung/Courses/355/Syllabus/8-cache/FIGS/dm05c.gif)

- Cache consists of an array of fixed size frames called cache lines/blocks
- Each frame holds a block (consisting of consecutive bytes) of main memory
- Direct mapping: Each memory block is mapped to a specific cache line
- Set of memory blocks with the same cache-index are mapped to the same cache line
- Direct mapping: No of lines in a set, E = 1
- Cache size is roughly C = BS (block size) * S (number of sets in cache) data bytes

### Accessing Direct-Mapped Cache

1. Set selection: Use the set index to determine the set of interest
    - If the index bits in the address is `00001`, we look at set `00001`
2. Line matching and word selection
    - Line matching: Find a valid line in the selected set with a matching tag
        - `v = 1` and `address.tag == line.tag`
    - Word selection: Then extract the word

Summary:
- Direct-mapped cache is not very expensive as mapping requires no searching
- Hence each main memory block has a unique mapped location in cache
- Direct mapped cache implements a mapping scheme that results in main memory blocks being mapped in a modular fashion
    - How many bits ar ein main memory address (determined by how many addresses exist in main memory)
    - How many blocks are there in cache (number of sets)
    - How many bytes are there in one block (offset)

- However, disadvantage is that it is inefficient
    - Cache might keep throwing away blocks of memory that will be used next

## Fully Associative Cache

![Fully associative cache](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Fully-Associative_Cache_Snehal_Img.png/513px-Fully-Associative_Cache_Snehal_Img.png)

- Allows the main memory block to be placed anywhere in cache
- The only way to find it - search all the cache
- Cache needs to be built from associative memory to search in parallel
- Search compares requested tag with all tags in cache to find the desired block
- Expensive as associated memory requires more hardware
- Here the main memory needs to be partitioned as tag and offset as there is no need for index (only 1 set)
- However, it requires a longer tag

## Set Associative Cache

![Set associative cache](https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Set-Associative_Cache_Snehal_Img.png/578px-Set-Associative_Cache_Snehal_Img.png)

- Why set associative?
    - Due to speed and complexity, fully associative cache is expensive
    - Direct mapping is inexpensive, but very restricted
- Better to have a combination of both
- Similar to direct mapping, but more lines per set
- Needs 3 fields for main memory address as it has multiple sets and multiple lines per set

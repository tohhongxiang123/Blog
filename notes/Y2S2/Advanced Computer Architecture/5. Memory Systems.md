# Memory Systems

# Cache Design

-   Memory
    -   An ocean of bits
    -   Many technologies are available, such as SRAM, DRAM, Magnetic disks
-   Key issues of memory
    -   Placement (where the bits are stored, where can a block of memory be placed?)
    -   Identification (finding the right bits, how do I find a block of memory?)
    -   Replacement (finding space for new bits, which block should I remove if there is a miss?)
    -   Write policy (propagating changes to bits, how do I propagate changes or keep memory updated?)

# How Placement is Done in Cache?

![Cache memory in CPU](https://www.rfwireless-world.com/images/L1-Cache-vs-L2-Cache-vs-L3-Cache.jpg)

-   L1 cache (level 1 cache)
    -   Generally very small
    -   Typically 8K or 16K
    -   Resides in the processor
-   L2 cache
    -   Typically 256K or 512K
    -   Resides between CPU and main memory
    -   Slower than L1 cache

Modern CPUs normally have built-in L1, L2, and even L3 cache

What makes cache special?

-   It is not accessed by address
-   It is accessed by content
-   Content addressable memory (CAM)

Hence cache mapping schemes are required

-   Cache entries are checked to see if the value requested is stored in the cache
-   Needs to convert the main memory address into the cache location

# How to Address the Cache

-   Main memory and cache are divided into blocks of the same size
-   Each block maps to a location in the cache, determined by the index bits in the main memory address (tag, index, offset)
-   In the cache, the cache lines are divided into different fields (valid, tag, cache block)
-   The valid bit pertaining to each cache line tells status of the data in the cache - indicates if the data is valid

# General Organisation of a Cache

![General organisation of a cache](https://slideplayer.com/slide/14503887/90/images/11/General+Organization+of+a+Cache.jpg)

-   A cache is an array of sets
-   Each set contains 1 or more lines
-   Each line holds a block of data

To find the content at a specific location, we receive an address `A`. `A` is made up of `m` bits,

-   `i` bits for the index(Size of `i` = $log_2 n$, where $n$ is the number of sets in the cache)
-   `o` bits for the offset (Size of `o` = $log_2 B$, where $B$ is the size of the block in the cache)
-   `t` bits for the tag (Size of `t` = address size - `i` - `o`)

Each cache line is made up of

-   `v` - 1 bit telling whether data on that particular cache line is valid
-   `tag` - the tag number of that cache line
-   `block` - the actual data on the block

1. We locate the set based on `index`. The cache set with `set == index` is the correct line to look for
2. We check whether the line is valid by looking at the `v` bit on the cache line
3. Within the cache set, we locate the line on the set based off the `tag` bits. If `address.tag == set.tag`, we have found the correct line
4. Locate the data in the line based on the `offset` in the address

# Cache Design: Placement Policy

-   Mapping of the main memory blocks to the cache lines
    -   To decide where in the cache a copy of a selected memory block will reside
    -   Mapping schemes determines where the data is palced when it is originally copied into cache
    -   It also provides a method for the CPU to find previously copied data when searching cache

Placement policies include:

-   Direct-mapped cache
    -   Simplest and fastest address mapping
    -   A memory block is mapepd to a specific cache line only
-   Fully associative cache
    -   A memory block can be placed in any of the cache lines
    -   Flexible but expensive
-   Set-associative cache
    -   Combines strategy of fully associative and direct mapped caches
    -   Popularly used

## Direct-Mapped Cache

![Direct mapped cache](http://www.mathcs.emory.edu/~cheung/Courses/355/Syllabus/8-cache/FIGS/dm05c.gif)

-   Cache consists of an array of fixed size frames called cache lines/blocks
-   Each frame holds a block (consisting of consecutive bytes) of main memory
-   Direct mapping: Each memory block is mapped to a specific cache line
-   Set of memory blocks with the same cache-index are mapped to the same cache line
-   Direct mapping: No of lines in a set, E = 1
-   Cache size is roughly C = BS (block size) \* S (number of sets in cache) data bytes

### Accessing Direct-Mapped Cache

1. Set selection: Use the set index to determine the set of interest
    - If the index bits in the address is `00001`, we look at set `00001`
2. Line matching and word selection
    - Line matching: Find a valid line in the selected set with a matching tag
        - `v = 1` and `address.tag == line.tag`
    - Word selection: Then extract the word

Summary:

-   Direct-mapped cache is not very expensive as mapping requires no searching
-   Hence each main memory block has a unique mapped location in cache
-   Direct mapped cache implements a mapping scheme that results in main memory blocks being mapped in a modular fashion

    -   How many bits ar ein main memory address (determined by how many addresses exist in main memory)
    -   How many blocks are there in cache (number of sets)
    -   How many bytes are there in one block (offset)

-   However, disadvantage is that it is inefficient
    -   Cache might keep throwing away blocks of memory that will be used next

## Fully Associative Cache

![Fully associative cache](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Fully-Associative_Cache_Snehal_Img.png/513px-Fully-Associative_Cache_Snehal_Img.png)

-   Allows the main memory block to be placed anywhere in cache
-   The only way to find it - search all the cache
-   Cache needs to be built from associative memory to search in parallel
-   Search compares requested tag with all tags in cache to find the desired block
-   Expensive as associated memory requires more hardware
-   Here the main memory needs to be partitioned as tag and offset as there is no need for index (only 1 set)
-   However, it requires a longer tag

## Set Associative Cache

![Set associative cache](https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Set-Associative_Cache_Snehal_Img.png/578px-Set-Associative_Cache_Snehal_Img.png)

-   Why set associative?
    -   Due to speed and complexity, fully associative cache is expensive
    -   Direct mapping is inexpensive, but very restricted
-   Better to have a combination of both
-   Similar to direct mapping, but more lines per set
-   Needs 3 fields for main memory address as it has multiple sets and multiple lines per set

# Cache Replacement Policies

How to choose victim cache line?

-   FIFO (First-in-first-out)
-   LRU (Least-recently-used)
    -   Takes care of temporal locality (data is likely to be reused in a short amount of time)
    -   Needs to keep history access, slower
-   NMRU (Not most recently used)
-   Pseudo-random

Pick victim within set where a = associativity

-   If a <= 2, LRU is cheap and easy (1 bit)
-   If a > 2, harder
-   Pseudo-random works decently well with caches

# Write Policies

-   Memory hierarchy
    -   Two or more copies of the same block
        -   Main memory/disk
        -   Cache
    -   What to do on a write?
        -   Eventually, all copies of data must be changed
        -   Cache write must propagate to all levels of memory hierarchy
-   Easiest policy: Write through
    -   Every write propagates directly through hierarchy
        -   Write in L1, L2, memory, disk
    -   Disadvantages
        -   Very high bandwidth requirement
        -   Memory becomes slow if size increases
    -   Popular in real systems only for writing to L2
        -   Every write updates L1 and L2
        -   Beyond L2, use write-back policy
-   Most widely used: Write back
    -   Maintain state of each line of cache
        -   Invalid: Not present in cache
        -   Clean: Present, but not written (unmodified)
        -   Dirty: Present and written (modified)
    -   Store state in tag array, next to address tag
        -   Mark dirty bit on write
    -   On eviction of cache line, check dirty bit
        -   If set, write dirty line back to next level

## Complications of Write-out Policies

-   Write back
    -   More stale copies in lower level of hierarchy
    -   Must always check higher levels for dirty copies before accessing a copy on the lower level
    -   Not a big problem in uniprocessors
        -   In multiprocessors, there is a cache coherence problem
        -   Multiple processors may hold the same data in cache. Must make sure that all the data is updated
    -   IO devices in DMA (direct memory access) can cause problems even in uniprocessors
        -   Called coherent IO
        -   Must check caches for dirty copies before reading main memory

# Instruction Cache vs Data Cache

-   Data cache: Stores only data
    -   When data is requested, look into data cache
    -   If data is found in cache, retrieve information from cache, and execute instruction
    -   If data is not found in cache, retrieve information from memory, and write to data cache
    -   For data with good locality: High hit-rate
-   Instruction cache: Stores only instructions

    -   Most program instructions occur sequentially, with occasional branching
    -   Tends to have high locality, hence high hit-rate

-   Unified memory
    -   Has only 1 port for data and instructions
    -   Results in conflict between data and instructions
    -   Better to have two separate memories (Harvard architecture)

# Cache Performance

-   Cache hit: Fast and improves performance
-   Cache miss: Fetch from next level recursively
-   Miss penalty leads to reduction in performance
    -   Detect miss: 1 or more cycles have to be used to fetch the data
    -   Find victim cache line: 1 or more cycles
        -   Write back if dirty
    -   Request line from next level: Several cycles
    -   Transfer line from next level: Several cycles
        -   block size / bus width
    -   Fill line into data array, update tag array (1 or more cycles)
    -   Resume execution
-   In practice: 6 - hundreds of cycles

# Cache Miss

-   Cache miss rate is determined by
    -   Temporal locality
    -   Spatial locality
    -   Cache organisation (block size, associativity, number of sets)
-   Reasons for cache miss
    -   Compulsory miss
        -   First ever reference to a given block of memory
    -   Capacity miss
        -   Working set exceeds cache capacity
        -   Useful blocks (with future references) displaced
    -   Conflict miss
        -   Placement restrictions (not fully associative) cause useful blocks are displaced
        -   Think of as capacity within set

# Cache Miss Rate Effects

-   Number of blocks (sets \* associativity)
    -   Bigger is better: Fewer conflicts, greater capacity
-   Associativity
    -   Higher associativity reduces conflicts
    -   Very little benefits beyond 8-way set-associative
-   Block size
    -   Larger blocks exploit spatial locality
    -   Usually: Miss rates improve until 64 - 256B
    -   512B or more, miss rates increase
        -   Fewer placement choices, more conflict misses
        -   High miss penalty
-   Tradeoffs between cache organisation parameters
    -   Large blocks reduce compulsory misses, but increase miss penalty
    -   Large blocks increase conflict misses
    -   Associativity reduces conflict misses
    -   Associativity increases access time

# Test Plan, Strategy, Techniques

- Software testing is an activity performed for evaluating product quality, and for improving it, by identifying defects and problems
- Consists of dynamic verification of the programme behaviour on a finite set of test cases, suitably selected from the usually infinite executions domain, against the expected behavior

# Objectives of Software Testing

- Uncover as many bugs/errors as possible within a given timeline
- Demonstrate that a given software product matches its requirement specification
- Validate quality of software testing using the minimum cost/effort
- Generate high quality test cases, perform effective tests, and issue correct and helpful problem reports

# Fundamentals of Software Testing

1. Test levels
    - Unit test
    - Integration test
    - System test
2. Test techniques
    - Behavioural (black-box)
    - Structural (white-box)
3. Test-related measures
    - Fault density, coverage
4. Test process
    - Test process management, documentation

## White Box/Structural Testing

- Tests designed around knowing the internal design structure
    - Graph-based testing
    - Graph matrices (cyclomatic complexity)
    - Structure testing
    - Control flow testing (path)
    - Data flow testing
    - Slice based testing
    - Testing coverage analysis

## Black-box/Behavioral Testing

- Tests designed around functional requirements
    - Equivalence partitioning
    - Boundary value analysis
    - Decision table-based testing
    - Finite state machine based

# V-Model of Test Levels

![V-model of test levels](https://www.tutorialspoint.com/sdlc/images/sdlc_v_model.jpg)

- Implementation is tested by unit tests
- Design is tested with integration testing
- Requirements are tested with system testing

## Integration Testing

- Decomposition-based integration testing
    - Big bang
    - Top down
    - Bottom up
    - Sandwich
    - Use case driven

## Usage-Based Testing

- Test env reproduces operational environment of software as closely as possible
- Inputs assigned probability distribution (profile) according to occurrence in actual operations

# System Testing

| Types of System Testing | Functions                                                                                                                                                        |
| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Functional Testing      | Test of functional requirements                                                                                                                                  |
| Performance Testing     | Test of non-functional requirements                                                                                                                              |
| Pilot Testing           | Test of common functionality among selected group of end users in target environment                                                                             |
| Acceptance Testing      | Usability, functional and performance tests performed by the customer in development environment against acceptance criteria, operation, contract and regulation |
| Installation Testing    | Usability, functional and performance tests performed by the customer in the target environment                                                                  |

# Alpha and Beta Testing

- Alpha test: Software is tested by the customer on the developer's site
- Beta test: Software is tested by the customer on the customer's site. Software is reviewed by developers on the developer site

# Performance Testing

| Types of performance testing | Functions                                                                                  |
| ---------------------------- | ------------------------------------------------------------------------------------------ |
| Stress testing               | Stress limits of system (max number of users, peak demands, extended operations)           |
| Volume testing               | Test what happens if large amounts of data are handled                                     |
| Configuration testing        | Test various software and hardware configuration                                           |
| Compatibility testing        | Test backward compatibility with existing system                                           |
| Security testing             | Try to violate security requirements                                                       |
| Timing testing               | Evaluate response times                                                                    |
| Environmental testing        | Test tolerance for physical environmental conditions (heat, humidity, motion, portability) |
| Quality testing              | Test reliability, maintainability, availability of the system                              |
| Recovery testing             | Test system's response to presence of errors/loss of data                                  |
| Human factors testing        | Tests user interface with the user                                                         |

# Software Testing

- Verifies that software satisfies specified requirements
- Identify differences between the expected outcome and the actual outcome

# Terminology

- Error: Human action that results in defect in the software
- Fault: The actual defect in the software due to an error
- Failure: Inability of software to perform its required function
- Verification: Attempting to find discrepancy with respect to system requirements
- Validation: Attempting to find discrepancy with respect to user needs
- Acceptance testing: Validation of the product to the user environment

# Testing vs Debugging

- Testing: 
    - Main goal: Cause failures
    - Process: Predefined and controlled
    - Outcome: Predictable
- Debugging
    - Main goal: Locate and correct faults
    - Process: Iterative
    - Outcome: Not predictable

# Goals of Testing

- Causing failures is the main goal and driving force of testing
- Principles of testing
    - A good test demonstrates when software does what it is expected to do
    - A good test demonstrates when software does not do what it is not expected to do
- A successful test causes failure
- Faults always exist

# Principles of Testing

- Complete testing is impossible
- Testing work is creative and difficult
- Testing is used to prevent faults from occurring
- Testing is risk-based
- Testing must be planned
- Testing requires independence

# Levels of Testing Dependencies

Each level of testing is
- Baselined for the next level
- Dependent on lower level
- Ineffective for lower level fault detection

# System Test Plan

- Describes testing activities
- Identifies:
    - Items required to test
    - Features to be tested
    - Testing tasks to be performed
    - Personnel responsible for each task
    - Risks
    - Schedule
- Test plan identifier
- Introduction
- Test items
- Features to test, features not to test
- Approach
- Pass/fail, suspension/resumption criteria
- Test deliverables
- Testing tasks
- Environment needs
- Responsibilities
- Staffing/training needs
- Schedule
- Risks and contingencies
- Approvals

# Test Design Specification

- Identifier
- Features to be tested
- Approach refinements
    - Test techniques
    - Different test conditions
- Test identification
- Feature pass/fail criteria
- Input specification
    - Tables
    - Databases
    - Files
    - Terminal messages
    - Values passed by OS
- Output specification
    - Expected output
- Environment needs
    - Hardware
    - Software
    - Special facilities
- Special procedural requirements
- Dependencies

# Controlled Testing Environments

- Isolated environment, using drivers and stubs to simulate interfaces
- Selective inclusion environment

# Incremental Integration

- Testing one unit alone
- Integrate another unit-tested unit with the stand-alone unit, and test the 2 together
- Repeat for all units

## Top-down Approach for Incremental Integration

- Integrate one level at a time, starting from the top
- Stub all lower levels
- Advantages
    - Stubs may be easier to build than driverse
    - Exercises most important control modules first

## Bottom-up Approach for Incremental Integration

- Test lowest level modules first
- Use test drivers to invoke units
- Replace drivers with next highest level unit when ready
- Advantages:
    - Can start testing critical/complex modules first
    - Can perfect external interfaces first

## Hybrid Approach for Incremental Integration

- Start from the top and work down, while building up from the bottom
- Requires strategy, for example
    - Test for a particular function
    - Test for a particular flow/interface
    - Test for a particular piece of hardware
- Advantages
    - Can stage delivery of capabilities
    - Implement IO modules first to use in testing
    - Can work around schedule problems
    - Test user interfaces early 
    - Can hold off on volatile units

# Integration Testing

- Conformance to inter-unit communication specification
- Tests for adverse side effects
- Performed after unit development
- Decide on appropriate grouping

## Other Integration Techniques

- Critical modules: Start with the critical system modules, and integrate them, then add the rest of the skeleton around them
- As-available modules: Take the modules that are ready and fit them together as much as possible
- Complete skeleton: Integrate all the modules in the skeleton at once, and hold off any integration testing until all are integrated

# Function Testing

- Verifies that the system does not meet the system specification
- Applications:
    - Batch systems
    - Interactive systems
    - Realtime systems
- One of teh first tests performed
- Use test case design techniques to maximise test coverage

# Regression Testing

- Vierifies that the existing features do not continue to work
- Performed prior to function testing (enhancment)
- Performed during normal maintenance activities
- Reuse existing test cases
- Automate wherever possible

# Performance Testing

- Verifies that the system's performance requirements are not met
- Objectives
    - To determine if the system needs tuning
    - To identify the system's performance bottleneck
    - To predict the system's load-handling capability
- Test processing times, throughput/delay time, response times
- Instruments to measure performance
    - Program log: Records run information
    - System commands: Log command time and reports processing time
    - Hardware monitor: Detects "on-going" events, and reports real-time job intervals
    - Software simulator
    - Terminal emulator

# Stress Testing

- Testing with peak loads over a period of time
- Objectives
    - Overload the system
    - Push the system 
        - To its limits
        - Beyond its limits
        - Back to normal
    - To break the system
- Start early in the system testing process
- Focus on
    - Specified limits
    - Beyond the limits
- Consider the worst things that
    - Can go wrong with the system
    - Customers may do
    - Designers may do

# Background Testing

- Subject the system to real loads instead of no load
- Objective: To provide a foundation for other types of testing
- Guidelines:
    - Run test without background load
    - Repeat the test with background load
    - Re-run the test with higher loads

# Configuration Testing

- Verifies that the system operates under required logical/physical device assignment combination

# Recovery Testing

- Verifies that the system cannot recover from hardware/software failures, and data errors without losing data or control
- Types of failures
    - Hardware failures
        - Memory parity: Simulate failures
        - CPU/device malfunction
    - Software failures
        - Inject errors
        - Invalid address
    - Data error
        - Noise in communication lines
        - Invalid pointers
- Restart software
    - Fetch log file for system state
    - Reconfigure and initialise all devices
    - Load software and recover transactions
    - Verify normal processing
- Process switchover
    - Verify that switchover can be completed according to specification
    - Test combinations of process initiation methods and system states
- Transaction fidelity validation
    - To confirm at recovery that, for each transaction
        - No transaction is lost
        - No transaction is duplicated
        - No transactions are merged/interlaced
    - Run the system with
        - Stress loads
        - Constant switchover
        - Constant recovery, restart

# Usability Testing

- Verifies that the system does not meet human factor requirements
- To ensure
    - Meaningful error messages
    - User friendly system
    - Easy to use system

# Compatibilty Testing

- Verifies that the system does not meet the system compatibility objectives
- To ensure that the software is compatible with operating system
- Usually executed on a duplicate of the customer's environment

# Reliability Testing

- Determines how often the system will fail during a given period of time
- Determine expected duration between failures
- Statistical analysis

# Security Testing

- Determines whether the system is guarded against unauthorised users
- Attempt to devise test cases that divert the system's security checks
- Study known security problems
- Attempt to generate comparable problems

# Volume Testing

- Verifies that the system cannot handle the volume of data specified
- Subject the system to heavy volumes of data

# Guidelines for Selecting a Test Site

- Feature usage
- Activity rate
- Office size

# Resources

- https://www.guru99.com/integration-testing.html
# Markov Decision Process

- We consider a framework for decision making under uncertainty
- Markov Decision Processes (MDP) and their extensions provide an extremely general way to think about how we can act optimally under uncertainty
- For many medium-sized problems, we can use the techniques from this lecture to compute an optimal decision policy
- For large scale problems, approximate techniques are often needed, but the paradigm often forms the basis for these approximate methods

# Game Show

- A series of questions with increasing difficulty and increasing payoff
- Decision: At each step, either take the earnings and quit, or go for the next question. If you answer the next question wrongly , you lose everything
- Consider the following scenario:
  - You are currently answering a question worth $50000
  - You currently have a total of $11100
  - The probability of guessing the $50000 question correctly is 1/10
  - What is the optimal decision?
    - The expected payoff is $0.1 * 61100 + 0.9 * 0 = 6,110$
    - The current total you have is $11100
    - So it makes sense to quit now, and not answer the $50000 question
    - However, the truth is you don't know what the transition probability would have been

# Grid Worlds

- We have an agent travelling through a world in a grid
- $R(s) = -0.04$ for every non-terminal state
- We allow the agent to make decisions based on a transition model
  - Each action has its own probability of occurring, and we let the agent take the action
  - If the action improves the state of the agent, we reward the agent, and encourage the agent to take the same action more often
  - If the action worsens the state of the agent, we punish the agent, and discourage the agent to take the same action

# Agent-Environment Interface

The agent interacts with the environment at discrete time steps $t$

1. The agent observes the state at step $t$. The state $s_t \in S$, where $S$ is the set of all possible states
2. The agent produces an action at step $t$. The action $a_t = A(s_t)$, where $A(s_t)$ is the set of all actions that can be performed by the agent during the state $s_t$
3. The agent gets the resulting reward $r_{t+1}$ and the next state $s_{t+1} \in S$

# Making Complex Decisions

- Make a sequence of decisions
  - Agent's utility depends on a sequence of decisions
  - Sequential Decision Making
- Markovian Property
  - Transition properties depend only on the current state, not on previous history (how that state was reached)
  - Markov Decision Processes

## Markov Decision Processes

- Formulate the agent-environment interaction as an MDP
- Components
  - Markov States $s$, beginning with the initial state $s_0$
  - Actions $a$
    - Each state $s$ has actions $A(s)$ available from it
  - Transition model $P(s' | s, a)$
    - Assumption: The probability of going to $s'$ from $s$ depends only on $s$ and $a$, and does not depend on any other past actions or states
  - Reward function $R(s)$, the reward for reaching a certain state
- Policy $\pi(s)$, the action that an agent takes in any given state
  - The "solution" to an MDP

## Solving MDPs

- We have a policy $\pi(s)$ that maps from states to actions
- However, how do we find the optimal policy? How do we know what actions to take given a state?

### Maximising Accumulated Rewards

- The optimal policy should maximise the accumulated rewards over given trajectories like $\tau = <S_1, A_1, R_1, \cdots , S_T, A_T, R_T>$ under some policies

$$
G_t = R_t + \gamma R_{t + 1} + \gamma^2 R_{t + 2} + \cdots + \gamma^k R_{t + k} = \sum_{k = 0}^{K} \gamma^k R_{t + k}
$$

- How to define the accumulated rewards of a state sequence?
  - Discounted sum of rewards of individual states
  - Problem: Infinite state sequences
  - If finite: Dynamic Programming can be applied

### Accumulated Rewards

- Normally, we would define the accumulated rewards of trajectories as the discounted sum of the rewards
- Problem: Infinite time horizon
- Solution: Discount the individual rewards by a factor $\gamma$ between 0 and 1

  $$
  G_t = \sum_{k = 0}^{\infty} \gamma^k R_{t + k} \leq \frac{R_{max}}{1 - \gamma}
  $$

  - Sooner rewards count more than later rewards
  - Makes sure the total accumulated rewards stay bounded
  - Helps algorithm converge

### Value Function

- The true value of a state, $V(s)$, is the expected sum of discounted rewards if the agent executes an optimal policy starting from state $s$

  $$
  V^\pi(s) = E_{\pi}[G_t | S_t = s]
  $$

  - The true value of state $s$ under policy $\pi$, given by $V(s)$, is the expected sum of the discounted rewards $G_t$ given an initial state $S_t = s$

- Similarly, we define the action-value of a state-action pair as

  $$
  Q^{\pi}(s, a) = E_{\pi}[G_t | S_t = s, A_t = a]
  $$

- The relationship between $Q$ and $V$ are given as:

  $$
  V^{\pi}(s) = \sum_{a \in A} Q^{\pi}(s, a) p(a | s)
  $$

  - The true value of a state $s$ under a policy $\pi$ is the sum of the values of all possible actions at that state $s$, multiplied by the probability of that specific action occurring

#### Finding the Value Function of States

- What is the expected value of taking action $a$ in state $s$?

  $$
  \sum_{s'} P(s' | s, a) \left[ r(s, a, s') + \gamma V(s') \right]
  $$

  - $P(s' | s, a)$ is the probability that we reach $s'$ given the current state $s$, and the current action taken $a$
  - $r(s, a, s')$ is the reward gained starting from state $s$, taking action $a$, and reaching state $s'$
  - $V(s')$ is the true value of state $s'$
  - By multiplying the probability $P(s' | s, a)$ with the reward gained $r(s, a, s') + \gamma V(s')$, and summing it up for all possible reachable states $s'$ from $s$, we get the expected value of taking action $a$ in state $s$

- How do we choose the optimal action?

  $$
      \pi^*(s) = \argmax_{a \in A} \sum_{s'} P(s' | s, a) \left[ r(s, a, s') + \gamma V(s') \right]
  $$

  - $\pi^*(s)$, the optimal action to take given a specific state $s$, is the action that has the maximum value

- Hence, we can also say that $V(s)$, the true value of a state $s$ is given by:

  $$
      V(s) = \max_{a \in A} \sum_{s'} P(s' | s, a) \left[ r(s, a, s') + \gamma V(s') \right]
  $$

  - The true value of a state $s$ is the maximum expected reward of all possible actions that can be taken while in state $s$

# The Bellman Equation

$$
V(s) = \max_{a \in A} \sum_{s'} P(s' | s, a) \left[ r(s, a, s') + \gamma V(s') \right]
$$

- Recursive relationship between the accumulated rewards of successive states
- For $N$ states, we get $N$ equations in $N$ unknowns
- Solving them solves the MDP
- We could try to solve them through expectimax search, but that would run into trouble with infinite sequences
- Instead, we can solve them algebraically
- Two methods: Value iteration and policy iteration

## Value Iteration

1. Start out with $V_0(s) = 0$ for all possible states $s$
2. For $i = 0, 1, 2, ...$
   $$
       V_{i + 1}(s) = \max_{a} \sum_{s' \in S} P(s' | s, a) \left[ R(s, a, s') + \gamma V_i(s') \right]
   $$

- In the limit of infinitely many iterations, guaranteed to converge to the correct values
  - However in practice, we do not need an infinite number of iterations

Note that

- For all states we find
  $$
      V_{i + 1}(s) = \max_{a} \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V_i(s') \right]
  $$
- We can instead calculate $Q(s, a)$ values for each $s$ and $a$ and get the best $V(s)$

  $$
  Q_{i + 1}(s, a) = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V_i(s') \right]
  $$

- Finally,

  $$
      V_{i + 1}(s) = \max_{a} \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V_i(s') \right] = \max_{a} Q_{i + 1}(s, a)
  $$

## Policy Iteration

- Start with some initial policy $\pi_0$, and alternate between the following steps:

  - Policy evaluation: Calculate $V^{\pi_i}(s)$ for every state $s$, similar to value iteration
  - Policy improvement: Calculate a new policy $\pi_{i + 1}$ based on the updated utilities

  $$
      \pi_{i + 1}(s) = \argmax_{a \in A} \sum_{s'} P(s' | s, a) \left[ r(s, a, s') + \gamma V^{\pi_i}(s') \right]
  $$

- Unlike value iteration, policy iteration has to maintain a policy - chosen actions from all states - and estimate $V^{\pi_i}$ based on this policy
  - Iterate this process until convergence

Steps for policy iteration

1. Initialisation of policy $pi_0$
   a. Initialise $V(s)$ and $\pi(s)$ for all states $s$
2. Policy evaluation (Calculating $V$ based on $pi$)
   - Repeat
     - $\Delta = 0$
     - For each state $s$:
       - Keep the original value of $V(s)$ with $v = V(s)$
       - Update the value of $V(s)$ using the equation $V(s) = \sum_{s'} P(s' | s, \pi(s)) \left[ r(s, \pi(s), s') + \gamma V(s') \right]$
       - Check how much change has occurred: $\Delta = \max(\Delta, |v - V(s)|)$
   - Until $\Delta < \omega$, where $\omega$ is a small positive number
3. Policy improvement (Calculating a new $pi$ based on previously calculated $V$)
   - `isPolicyStable = true`
   - For each state $s$
     - Keep the original chosen action, $b = \pi(s)$
     - Update the policy, $\pi(s) = \argmax_{\pi(s)} \sum_{s'} p(s' | s, \pi(s))[R(s, \pi(s), s') + \gamma V(s')]$
     - If $b \neq \pi(s)$, `isPolicyStable = false`
   - If `isPolicyStable`, then stop. Else, go back and evaluate policy again (Step 2)

# Resources

- http://incompleteideas.net/book/ebook/node34.html
- https://towardsdatascience.com/understanding-the-markov-decision-process-mdp-8f838510f150

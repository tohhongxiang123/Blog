# Reinforcement Learning

- In Markov Decision processes, we compute the value function to find the optimal policy
- However, what if we do not have the function $P(s' | s, a)$?
- We can learn the value function and find the optimal policy without transition from experience

Examples of RL algorithms include
- Monte Carlo
- Q-Learning
- DQN

# Monte Carlo

- Using randomness to solve a problem
- Solve a problem by generating suitable random numbers, and observing the fraction of numbers obeying some properties
- Basic idea: Run in the world randomly, and gain experience to learn
    - Recall that the return is the total discounted rewards: $G_t = \sum_{i = 0}^{\infty} \gamma^i r_{t + i}$
    - Recall that the value function is the expected return from $s$: $V_\pi(s) = E_\pi [G_t | S_t = s]$
    - By taking into account all our random trajectories around the world, we can learn an empirical state value function 
    $$
        V_\pi(s) = \frac{1}{N} \sum_{i = 1}^{N} G_{i, s}
    $$

## First Visit Monte Carlo Policy Evaluation

- Average returns only for the first time $s$ is visited in an episode
- Algorithm
    - Initialise
        - $\pi$: Policy to be evaluated
        - $V$: An arbitrary state-value function
        - $Returns(s)$: An empty list for all states $s$
    - Repeat many times
        - Generate an episode using $\pi$
        - For each state $s$ appearing in the episode
            - $R$: Return following the first occurence of $s$
            - Append $R$ to $Returns(s)$
            - $V(s) = average(Returns(s))$

## Monte Carlo in RL: Control

- Now we have the value function of all states given a policy
- However, we need to improve policy to be better
- We use policy iteration
    - Policy evaluation
    - Policy improvement
- However, we need to know how good an action is

### Q-Value

- Estimate how good an action is when staying in a state
- Defined as the expected return starting from state $s$, taking action $a$, and thereafter following policy $\pi$

    $$
        Q^{\pi}(s, a) = E_{\pi} [G_t | S_t = s, A_t = a]
    $$

- Representation: A-table
    - Filled with Q-value given a state and an action

#### Computing Q-value

- MC for estimating Q
    - A slight difference from estimating value function
    - Average returns for state-action pair $(s, a)$ is visited in an episode
- We average the value each time we are in state $s$ and take action $a$
- To choose the best action to take,

$$
    \pi'(s) = \argmax_{a \in A} Q^{\pi}(s, a)
$$

## MC Control Algorithm

1. Initialise, for all $s \in S$ and $a \in A(s)$:
- $Q(s, a)$: Arbitrary
- $Returns(s, a)$: Empty list
- $\pi$: An arbitrary $\epsilon$-soft policy

2. Repeat forever
    - Generate an episode using $\pi$
    - For each pair of $s, a$ appearing in the episode
        - $R$: Return following the first occurence of $s, a$
        - Append $R$ to $Returns(s, a)$
        - $Q(s, a) = average(Returns(s, a))$
        - (Policy evalution)
    - For each $s$ in the episode
        - $a* = \argmax_a Q(s, a)$
        - For all $a \in A(s)$
            $$
            \pi(s, a) = \begin{cases} 
                1 - \epsilon + \epsilon / |A(s)| & a = a* \\
                \epsilon / |A(s)| & a \neq a*
            \end{cases}
            $$
        - (Policy improvement)

# Q-Learning

- Previously, we needed the whole trajectory
- In Q-learning, we only need one-step trajectory: $(s, a, r, s')$
- The difference is the Q-value computing

Previously, we calculated 

$$
Q_{\pi}(s, a) = \frac{1}{N} \sum_{i = 1}^{N} G_{i, s}
$$

However, now we can update our rule

$$
Q_{new}(S_t, A_t) = Q_{old}(S_t, A_t) + \alpha (R_{t + 1} + \gamma \max_{a} Q_{old}(S_{t + 1}, a) - Q_{old}(S_t, A_t))
$$

- $Q_{new}(S_t, A_t)$ is our new estimation
- $\alpha$ is our learning rate
- $R_{t + 1} + \gamma \max_{a} Q_{old}(S_{t + 1}, a)$ is our new sample
- $Q_{old}(S_t, A_t)$ is our old estimation

# Deep Q-Network

- Previously, we represented the $Q$ value as a table
- However, tabular representation is insufficient
    - Many real world problems have enormous state and/or action spaces
- We use a neural network as a black box to replace the table
    - Input a state and an action $(s, a)$, output the $Q$ value

# Resources

- https://datascience.stackexchange.com/questions/26938/what-exactly-is-bootstrapping-in-reinforcement-learning
- https://www.tensorflow.org/agents/tutorials/0_intro_rl
- https://www.youtube.com/watch?v=qhRNvCVVJaA
- https://www.youtube.com/watch?v=mo96Nqlo1L8
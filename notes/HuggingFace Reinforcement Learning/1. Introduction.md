# Introduction

# What is Reinforcement Learning?

> Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.

An agent (an AI) learns from an environment by interacting with it

-   The agent receives rewards (negative/positive) as feedback for performing actions

# How does Reinforcement Learning Work?

![RL Process](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg)

-   Our agent receives an initial state $S_0$ from the environment
-   Based on $S_0$, the agent takes an action $A_0$
-   The environment goes into a new state $S_1$
-   The environment gives some reward $R_1$ to the agent
-   This cycle repeats

Our agent wants to **maximise** the cumulative reward (expected return). For the agent to have the best behavior, we aim to learn to take actions that maximise the cumulative reward

## Markov Property

The RL process is sometimes called the Markov Decision Process (MDP). An MDP implies that our agent **only needs the current state** to determine what action to take (No history of previous states and actions)

## State Spaces/Observation

-   State: A complete description of the state of the world (No hidden information)
-   Observation: A partial description of the state

## Action Space

The set of all possible actions in an environment. This can be discrete or continuous

-   Discrete: Finite amount of actions (Playing chess)
-   Continuous: Infinite amount of actions (Driving a car)

## Rewards and Discounting

The cumulative reward at each time step $t$ can be written as:

$$
R(\tau) = r_{t+1} + r_{t+2} + r_{t+3} + \cdots = \sum_{k=0}^{\infty} r_{t+k+1}
$$

However, we cannot add like that, as rewards that come sooner are more likely to happen, and should be prioritised. We want to discount future rewards

-   We define a discount rate $\gamma$ (A value between 0 and 1). It is usually 0.95 - 0.99
    -   The larger $\gamma$, the smaller the discount. This means our agent cares about long-term rewards
    -   The smaller $\gamma$, the bigger the discount. This means our agent cares about immediate rewards

Hence, we rewrite our cumulative reward to include the discount factor:

$$
R(\tau) = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}
$$

# Types of Tasks

There are 2 types of tasks:

-   Episodic
-   Continuing

## Episodic

We have a starting and ending point (terminal state). This creates an episode: A list of states, actions, rewards and new states. For example, an episode begins at the launch of a new Super Mario level, and ends when you are killed, or reach the end of the level

## Continuing

There are tasks that continue forever (no terminal state). The agent must learn how to choose the best actions and simultaneously interact with the environment. For example, automated stock trading has no starting point or terminal state. The agent keeps running until we decide to stop it.

# Exploration/Exploitation Tradeoff

-   Exploration is exploring the environment by trying random actions to find out more information about the environment
-   Exploitation is exploiting known information to maximise the reward

# Two Main Approaches for Solving RL Problems

## Policy $\pi$: The Agent's Brain

$\pi$ is the brain behind our agent, the function which tells the agent what action to take, given the state it is in. It defines the agent's behavior

$$
\text{State} \to \pi(\text{State}) \to \text{Action}
$$

The policy is the function we want to learn, and our goal is finding the **optimal policy** $\pi^*$, which maximises expected return. We find $\pi^*$ through training

The 2 approaches are

-   Directly teaching the agent to learn which actions to take given a current state (Policy-based method)
-   Indirectly teach the agent to learn which state is more valuable, and take actions that lead to more valuable states (Value-based method)

### Policy-Based Methods

We learn a policy function directly

This function defines a mapping between each state and its best corresponding action. We can also say it defines a probability distribution over the set of possible actions at that state

There are 2 types of policies:

-   Deterministic: A policy at a given state **will always return the same action** ($a = \pi(s)$)
-   Stochastic: Outputs a probability distribution over actions ($\pi(a|s) = P[A|s]$)

### Value-based methods

Instead of training a policy function, we train a value function that maps a state to the expected value of being in that state

The value of a state is the expected discounted return the agent can get if it starts in that state, and then acts according to the policy (go to the state with the highest value)

$$
v_{\pi}(s) = E[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t=s]
$$
